{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af12250e-0d9a-4294-b20d-d370e35013f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:16.961331Z",
     "iopub.status.busy": "2024-07-20T13:55:16.961051Z",
     "iopub.status.idle": "2024-07-20T13:55:18.305765Z",
     "shell.execute_reply": "2024-07-20T13:55:18.305419Z",
     "shell.execute_reply.started": "2024-07-20T13:55:16.961316Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn import over_sampling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, precision_recall_fscore_support, precision_recall_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from skopt import BayesSearchCV\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1d508-7299-4317-b29c-99611e3b9c12",
   "metadata": {},
   "source": [
    "## Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f4cd61-a3c7-48e5-b090-fdd21e0c79fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:19.407455Z",
     "iopub.status.busy": "2024-07-20T13:55:19.407174Z",
     "iopub.status.idle": "2024-07-20T13:55:20.712251Z",
     "shell.execute_reply": "2024-07-20T13:55:20.711922Z",
     "shell.execute_reply.started": "2024-07-20T13:55:19.407442Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.read_csv(\"../data/air_system_previous_years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0b0536-eeb8-4d04-9212-bf0380680ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:21.209998Z",
     "iopub.status.busy": "2024-07-20T13:55:21.209718Z",
     "iopub.status.idle": "2024-07-20T13:55:21.244189Z",
     "shell.execute_reply": "2024-07-20T13:55:21.243886Z",
     "shell.execute_reply.started": "2024-07-20T13:55:21.209985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>ag_004</th>\n",
       "      <th>ag_005</th>\n",
       "      <th>ag_006</th>\n",
       "      <th>ag_007</th>\n",
       "      <th>ag_008</th>\n",
       "      <th>ag_009</th>\n",
       "      <th>ah_000</th>\n",
       "      <th>ai_000</th>\n",
       "      <th>aj_000</th>\n",
       "      <th>ak_000</th>\n",
       "      <th>al_000</th>\n",
       "      <th>am_0</th>\n",
       "      <th>an_000</th>\n",
       "      <th>ao_000</th>\n",
       "      <th>ap_000</th>\n",
       "      <th>aq_000</th>\n",
       "      <th>ar_000</th>\n",
       "      <th>as_000</th>\n",
       "      <th>at_000</th>\n",
       "      <th>au_000</th>\n",
       "      <th>av_000</th>\n",
       "      <th>ax_000</th>\n",
       "      <th>ay_000</th>\n",
       "      <th>ay_001</th>\n",
       "      <th>ay_002</th>\n",
       "      <th>ay_003</th>\n",
       "      <th>ay_004</th>\n",
       "      <th>ay_005</th>\n",
       "      <th>ay_006</th>\n",
       "      <th>ay_007</th>\n",
       "      <th>ay_008</th>\n",
       "      <th>ay_009</th>\n",
       "      <th>az_000</th>\n",
       "      <th>az_001</th>\n",
       "      <th>az_002</th>\n",
       "      <th>az_003</th>\n",
       "      <th>az_004</th>\n",
       "      <th>az_005</th>\n",
       "      <th>az_006</th>\n",
       "      <th>az_007</th>\n",
       "      <th>az_008</th>\n",
       "      <th>az_009</th>\n",
       "      <th>ba_000</th>\n",
       "      <th>ba_001</th>\n",
       "      <th>ba_002</th>\n",
       "      <th>ba_003</th>\n",
       "      <th>ba_004</th>\n",
       "      <th>ba_005</th>\n",
       "      <th>ba_006</th>\n",
       "      <th>ba_007</th>\n",
       "      <th>ba_008</th>\n",
       "      <th>ba_009</th>\n",
       "      <th>bb_000</th>\n",
       "      <th>bc_000</th>\n",
       "      <th>bd_000</th>\n",
       "      <th>be_000</th>\n",
       "      <th>bf_000</th>\n",
       "      <th>bg_000</th>\n",
       "      <th>bh_000</th>\n",
       "      <th>bi_000</th>\n",
       "      <th>bj_000</th>\n",
       "      <th>bk_000</th>\n",
       "      <th>bl_000</th>\n",
       "      <th>bm_000</th>\n",
       "      <th>bn_000</th>\n",
       "      <th>bo_000</th>\n",
       "      <th>bp_000</th>\n",
       "      <th>bq_000</th>\n",
       "      <th>br_000</th>\n",
       "      <th>bs_000</th>\n",
       "      <th>bt_000</th>\n",
       "      <th>bu_000</th>\n",
       "      <th>bv_000</th>\n",
       "      <th>bx_000</th>\n",
       "      <th>by_000</th>\n",
       "      <th>bz_000</th>\n",
       "      <th>ca_000</th>\n",
       "      <th>cb_000</th>\n",
       "      <th>cc_000</th>\n",
       "      <th>cd_000</th>\n",
       "      <th>ce_000</th>\n",
       "      <th>cf_000</th>\n",
       "      <th>cg_000</th>\n",
       "      <th>ch_000</th>\n",
       "      <th>ci_000</th>\n",
       "      <th>cj_000</th>\n",
       "      <th>ck_000</th>\n",
       "      <th>cl_000</th>\n",
       "      <th>cm_000</th>\n",
       "      <th>cn_000</th>\n",
       "      <th>cn_001</th>\n",
       "      <th>cn_002</th>\n",
       "      <th>cn_003</th>\n",
       "      <th>cn_004</th>\n",
       "      <th>cn_005</th>\n",
       "      <th>cn_006</th>\n",
       "      <th>cn_007</th>\n",
       "      <th>cn_008</th>\n",
       "      <th>cn_009</th>\n",
       "      <th>co_000</th>\n",
       "      <th>cp_000</th>\n",
       "      <th>cq_000</th>\n",
       "      <th>cr_000</th>\n",
       "      <th>cs_000</th>\n",
       "      <th>cs_001</th>\n",
       "      <th>cs_002</th>\n",
       "      <th>cs_003</th>\n",
       "      <th>cs_004</th>\n",
       "      <th>cs_005</th>\n",
       "      <th>cs_006</th>\n",
       "      <th>cs_007</th>\n",
       "      <th>cs_008</th>\n",
       "      <th>cs_009</th>\n",
       "      <th>ct_000</th>\n",
       "      <th>cu_000</th>\n",
       "      <th>cv_000</th>\n",
       "      <th>cx_000</th>\n",
       "      <th>cy_000</th>\n",
       "      <th>cz_000</th>\n",
       "      <th>da_000</th>\n",
       "      <th>db_000</th>\n",
       "      <th>dc_000</th>\n",
       "      <th>dd_000</th>\n",
       "      <th>de_000</th>\n",
       "      <th>df_000</th>\n",
       "      <th>dg_000</th>\n",
       "      <th>dh_000</th>\n",
       "      <th>di_000</th>\n",
       "      <th>dj_000</th>\n",
       "      <th>dk_000</th>\n",
       "      <th>dl_000</th>\n",
       "      <th>dm_000</th>\n",
       "      <th>dn_000</th>\n",
       "      <th>do_000</th>\n",
       "      <th>dp_000</th>\n",
       "      <th>dq_000</th>\n",
       "      <th>dr_000</th>\n",
       "      <th>ds_000</th>\n",
       "      <th>dt_000</th>\n",
       "      <th>du_000</th>\n",
       "      <th>dv_000</th>\n",
       "      <th>dx_000</th>\n",
       "      <th>dy_000</th>\n",
       "      <th>dz_000</th>\n",
       "      <th>ea_000</th>\n",
       "      <th>eb_000</th>\n",
       "      <th>ec_00</th>\n",
       "      <th>ed_000</th>\n",
       "      <th>ee_000</th>\n",
       "      <th>ee_001</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>76698</td>\n",
       "      <td>na</td>\n",
       "      <td>2130706438</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37250</td>\n",
       "      <td>1432864</td>\n",
       "      <td>3664156</td>\n",
       "      <td>1007684</td>\n",
       "      <td>25896</td>\n",
       "      <td>0</td>\n",
       "      <td>2551696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4933296</td>\n",
       "      <td>3655166</td>\n",
       "      <td>1766008</td>\n",
       "      <td>1132040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1012</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469014</td>\n",
       "      <td>4239660</td>\n",
       "      <td>703300</td>\n",
       "      <td>755876</td>\n",
       "      <td>0</td>\n",
       "      <td>5374</td>\n",
       "      <td>2108</td>\n",
       "      <td>4114</td>\n",
       "      <td>12348</td>\n",
       "      <td>615248</td>\n",
       "      <td>5526276</td>\n",
       "      <td>2378</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2328746</td>\n",
       "      <td>1022304</td>\n",
       "      <td>415432</td>\n",
       "      <td>287230</td>\n",
       "      <td>310246</td>\n",
       "      <td>681504</td>\n",
       "      <td>1118814</td>\n",
       "      <td>3574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6700214</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>108</td>\n",
       "      <td>50</td>\n",
       "      <td>2551696</td>\n",
       "      <td>97518</td>\n",
       "      <td>947550</td>\n",
       "      <td>799478</td>\n",
       "      <td>330760</td>\n",
       "      <td>353400</td>\n",
       "      <td>299160</td>\n",
       "      <td>305200</td>\n",
       "      <td>283680</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>178540</td>\n",
       "      <td>76698.08</td>\n",
       "      <td>6700214</td>\n",
       "      <td>6700214</td>\n",
       "      <td>6599892</td>\n",
       "      <td>43566</td>\n",
       "      <td>68656</td>\n",
       "      <td>54064</td>\n",
       "      <td>638360</td>\n",
       "      <td>6167850</td>\n",
       "      <td>1209600</td>\n",
       "      <td>246244</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>5245752</td>\n",
       "      <td>0</td>\n",
       "      <td>916567.68</td>\n",
       "      <td>6</td>\n",
       "      <td>1924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118196</td>\n",
       "      <td>1309472</td>\n",
       "      <td>3247182</td>\n",
       "      <td>1381362</td>\n",
       "      <td>98822</td>\n",
       "      <td>11208</td>\n",
       "      <td>1608</td>\n",
       "      <td>220</td>\n",
       "      <td>240</td>\n",
       "      <td>6700214</td>\n",
       "      <td>na</td>\n",
       "      <td>10476</td>\n",
       "      <td>1226</td>\n",
       "      <td>267998</td>\n",
       "      <td>521832</td>\n",
       "      <td>428776</td>\n",
       "      <td>4015854</td>\n",
       "      <td>895240</td>\n",
       "      <td>26330</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>532</td>\n",
       "      <td>734</td>\n",
       "      <td>4122704</td>\n",
       "      <td>51288</td>\n",
       "      <td>0</td>\n",
       "      <td>532572</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>5330690</td>\n",
       "      <td>4732</td>\n",
       "      <td>1126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62282</td>\n",
       "      <td>85908</td>\n",
       "      <td>32790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202710</td>\n",
       "      <td>37928</td>\n",
       "      <td>14745580</td>\n",
       "      <td>1876644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2801180</td>\n",
       "      <td>2445.8</td>\n",
       "      <td>2712</td>\n",
       "      <td>965866</td>\n",
       "      <td>1706908</td>\n",
       "      <td>1240520</td>\n",
       "      <td>493384</td>\n",
       "      <td>721044</td>\n",
       "      <td>469792</td>\n",
       "      <td>339156</td>\n",
       "      <td>157956</td>\n",
       "      <td>73224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>33058</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18254</td>\n",
       "      <td>653294</td>\n",
       "      <td>1720800</td>\n",
       "      <td>516724</td>\n",
       "      <td>31642</td>\n",
       "      <td>0</td>\n",
       "      <td>1393352</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2560898</td>\n",
       "      <td>2127150</td>\n",
       "      <td>1084598</td>\n",
       "      <td>338544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71510</td>\n",
       "      <td>772720</td>\n",
       "      <td>1996924</td>\n",
       "      <td>99560</td>\n",
       "      <td>0</td>\n",
       "      <td>7336</td>\n",
       "      <td>7808</td>\n",
       "      <td>13776</td>\n",
       "      <td>13086</td>\n",
       "      <td>1010074</td>\n",
       "      <td>1873902</td>\n",
       "      <td>14726</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1378576</td>\n",
       "      <td>447166</td>\n",
       "      <td>199512</td>\n",
       "      <td>154298</td>\n",
       "      <td>137280</td>\n",
       "      <td>138668</td>\n",
       "      <td>165908</td>\n",
       "      <td>229652</td>\n",
       "      <td>87082</td>\n",
       "      <td>4708</td>\n",
       "      <td>3646660</td>\n",
       "      <td>86</td>\n",
       "      <td>454</td>\n",
       "      <td>364</td>\n",
       "      <td>350</td>\n",
       "      <td>1393352</td>\n",
       "      <td>49028</td>\n",
       "      <td>688314</td>\n",
       "      <td>392208</td>\n",
       "      <td>341420</td>\n",
       "      <td>359780</td>\n",
       "      <td>366560</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>6700</td>\n",
       "      <td>33057.51</td>\n",
       "      <td>3646660</td>\n",
       "      <td>3646660</td>\n",
       "      <td>3582034</td>\n",
       "      <td>17733.0</td>\n",
       "      <td>260120</td>\n",
       "      <td>115626</td>\n",
       "      <td>6900</td>\n",
       "      <td>2942850</td>\n",
       "      <td>1209600</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>2291079.36</td>\n",
       "      <td>0</td>\n",
       "      <td>643536.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>98644</td>\n",
       "      <td>1179502</td>\n",
       "      <td>1286736</td>\n",
       "      <td>336388</td>\n",
       "      <td>36294</td>\n",
       "      <td>5192</td>\n",
       "      <td>56</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>3646660</td>\n",
       "      <td>na</td>\n",
       "      <td>6160</td>\n",
       "      <td>796</td>\n",
       "      <td>164860</td>\n",
       "      <td>350066</td>\n",
       "      <td>272956</td>\n",
       "      <td>1837600</td>\n",
       "      <td>301242</td>\n",
       "      <td>9148</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>3312</td>\n",
       "      <td>522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33736</td>\n",
       "      <td>36946</td>\n",
       "      <td>5936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103330</td>\n",
       "      <td>16254</td>\n",
       "      <td>4510080</td>\n",
       "      <td>868538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477820</td>\n",
       "      <td>2211.76</td>\n",
       "      <td>2334</td>\n",
       "      <td>664504</td>\n",
       "      <td>824154</td>\n",
       "      <td>421400</td>\n",
       "      <td>178064</td>\n",
       "      <td>293306</td>\n",
       "      <td>245416</td>\n",
       "      <td>133654</td>\n",
       "      <td>81140</td>\n",
       "      <td>97576</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>41040</td>\n",
       "      <td>na</td>\n",
       "      <td>228</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1648</td>\n",
       "      <td>370592</td>\n",
       "      <td>1883374</td>\n",
       "      <td>292936</td>\n",
       "      <td>12016</td>\n",
       "      <td>0</td>\n",
       "      <td>1234132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2371990</td>\n",
       "      <td>2173634</td>\n",
       "      <td>300796</td>\n",
       "      <td>153698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>358</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>870456</td>\n",
       "      <td>239798</td>\n",
       "      <td>1450312</td>\n",
       "      <td>0</td>\n",
       "      <td>1620</td>\n",
       "      <td>1156</td>\n",
       "      <td>1228</td>\n",
       "      <td>34250</td>\n",
       "      <td>1811606</td>\n",
       "      <td>710672</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>790690</td>\n",
       "      <td>672026</td>\n",
       "      <td>332340</td>\n",
       "      <td>254892</td>\n",
       "      <td>189596</td>\n",
       "      <td>135758</td>\n",
       "      <td>103552</td>\n",
       "      <td>81666</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>2673338</td>\n",
       "      <td>128</td>\n",
       "      <td>202</td>\n",
       "      <td>576</td>\n",
       "      <td>4</td>\n",
       "      <td>1234132</td>\n",
       "      <td>28804</td>\n",
       "      <td>160176</td>\n",
       "      <td>139730</td>\n",
       "      <td>137160</td>\n",
       "      <td>130640</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>28000</td>\n",
       "      <td>41040.08</td>\n",
       "      <td>2673338</td>\n",
       "      <td>2673338</td>\n",
       "      <td>2678534</td>\n",
       "      <td>15439.0</td>\n",
       "      <td>7466</td>\n",
       "      <td>22436</td>\n",
       "      <td>248240</td>\n",
       "      <td>2560566</td>\n",
       "      <td>1209600</td>\n",
       "      <td>63328</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>2322692.16</td>\n",
       "      <td>0</td>\n",
       "      <td>236099.52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33276</td>\n",
       "      <td>1215280</td>\n",
       "      <td>1102798</td>\n",
       "      <td>196502</td>\n",
       "      <td>10260</td>\n",
       "      <td>2422</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2673338</td>\n",
       "      <td>na</td>\n",
       "      <td>3584</td>\n",
       "      <td>500</td>\n",
       "      <td>56362</td>\n",
       "      <td>149726</td>\n",
       "      <td>100326</td>\n",
       "      <td>1744838</td>\n",
       "      <td>488302</td>\n",
       "      <td>16682</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>292</td>\n",
       "      <td>2180528</td>\n",
       "      <td>29188</td>\n",
       "      <td>22</td>\n",
       "      <td>20346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2341048</td>\n",
       "      <td>1494</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13876</td>\n",
       "      <td>38182</td>\n",
       "      <td>8138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65772</td>\n",
       "      <td>10534</td>\n",
       "      <td>300240</td>\n",
       "      <td>48028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1040120</td>\n",
       "      <td>1018.64</td>\n",
       "      <td>1020</td>\n",
       "      <td>262032</td>\n",
       "      <td>453378</td>\n",
       "      <td>277378</td>\n",
       "      <td>159812</td>\n",
       "      <td>423992</td>\n",
       "      <td>409564</td>\n",
       "      <td>320746</td>\n",
       "      <td>158022</td>\n",
       "      <td>95128</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>2212</td>\n",
       "      <td>3232</td>\n",
       "      <td>1872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>642</td>\n",
       "      <td>3894</td>\n",
       "      <td>10184</td>\n",
       "      <td>7554</td>\n",
       "      <td>10764</td>\n",
       "      <td>1014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2038</td>\n",
       "      <td>5596</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>914</td>\n",
       "      <td>76</td>\n",
       "      <td>2478</td>\n",
       "      <td>2398</td>\n",
       "      <td>1692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6176</td>\n",
       "      <td>340</td>\n",
       "      <td>304</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>406</td>\n",
       "      <td>216</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21614</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2668</td>\n",
       "      <td>184</td>\n",
       "      <td>7632</td>\n",
       "      <td>3090</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>10580</td>\n",
       "      <td>12.69</td>\n",
       "      <td>21614</td>\n",
       "      <td>21614</td>\n",
       "      <td>21772</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>1994</td>\n",
       "      <td>21400</td>\n",
       "      <td>7710</td>\n",
       "      <td>1209600</td>\n",
       "      <td>302</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2135.04</td>\n",
       "      <td>0</td>\n",
       "      <td>4525.44</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2544</td>\n",
       "      <td>1894</td>\n",
       "      <td>2170</td>\n",
       "      <td>822</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21614</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>656</td>\n",
       "      <td>692</td>\n",
       "      <td>4836</td>\n",
       "      <td>388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>8</td>\n",
       "      <td>1666</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2578</td>\n",
       "      <td>76</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>370</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>15740</td>\n",
       "      <td>1822</td>\n",
       "      <td>20174</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>54</td>\n",
       "      <td>5670</td>\n",
       "      <td>1566</td>\n",
       "      <td>240</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>60874</td>\n",
       "      <td>na</td>\n",
       "      <td>1368</td>\n",
       "      <td>458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43752</td>\n",
       "      <td>1966618</td>\n",
       "      <td>1800340</td>\n",
       "      <td>131646</td>\n",
       "      <td>4588</td>\n",
       "      <td>0</td>\n",
       "      <td>1974038</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3230626</td>\n",
       "      <td>2618878</td>\n",
       "      <td>1058136</td>\n",
       "      <td>551022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1788</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42124</td>\n",
       "      <td>372236</td>\n",
       "      <td>2128914</td>\n",
       "      <td>819596</td>\n",
       "      <td>584074</td>\n",
       "      <td>0</td>\n",
       "      <td>1644</td>\n",
       "      <td>362</td>\n",
       "      <td>562</td>\n",
       "      <td>842</td>\n",
       "      <td>30194</td>\n",
       "      <td>3911734</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1348578</td>\n",
       "      <td>1035668</td>\n",
       "      <td>338762</td>\n",
       "      <td>236540</td>\n",
       "      <td>182278</td>\n",
       "      <td>151778</td>\n",
       "      <td>163248</td>\n",
       "      <td>470800</td>\n",
       "      <td>19292</td>\n",
       "      <td>0</td>\n",
       "      <td>4289260</td>\n",
       "      <td>448</td>\n",
       "      <td>556</td>\n",
       "      <td>642</td>\n",
       "      <td>2</td>\n",
       "      <td>1974038</td>\n",
       "      <td>86454</td>\n",
       "      <td>653692</td>\n",
       "      <td>399410</td>\n",
       "      <td>306780</td>\n",
       "      <td>282560</td>\n",
       "      <td>274180</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>189000</td>\n",
       "      <td>60874.03</td>\n",
       "      <td>4289260</td>\n",
       "      <td>4289260</td>\n",
       "      <td>4283332</td>\n",
       "      <td>24793.0</td>\n",
       "      <td>17052</td>\n",
       "      <td>61844</td>\n",
       "      <td>654700</td>\n",
       "      <td>3946944</td>\n",
       "      <td>1209600</td>\n",
       "      <td>135720</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>3565684.8</td>\n",
       "      <td>0</td>\n",
       "      <td>379111.68</td>\n",
       "      <td>0</td>\n",
       "      <td>746</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>356</td>\n",
       "      <td>378910</td>\n",
       "      <td>2497104</td>\n",
       "      <td>993000</td>\n",
       "      <td>64230</td>\n",
       "      <td>10482</td>\n",
       "      <td>2776</td>\n",
       "      <td>86</td>\n",
       "      <td>202</td>\n",
       "      <td>212</td>\n",
       "      <td>4289260</td>\n",
       "      <td>na</td>\n",
       "      <td>3942</td>\n",
       "      <td>520</td>\n",
       "      <td>80950</td>\n",
       "      <td>227322</td>\n",
       "      <td>186242</td>\n",
       "      <td>2288268</td>\n",
       "      <td>1137268</td>\n",
       "      <td>22228</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>1716</td>\n",
       "      <td>1664</td>\n",
       "      <td>3440288</td>\n",
       "      <td>215826</td>\n",
       "      <td>0</td>\n",
       "      <td>4262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3590004</td>\n",
       "      <td>2026</td>\n",
       "      <td>444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44946</td>\n",
       "      <td>62648</td>\n",
       "      <td>11506</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>149474</td>\n",
       "      <td>35154</td>\n",
       "      <td>457040</td>\n",
       "      <td>80482</td>\n",
       "      <td>98334</td>\n",
       "      <td>27588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21173050</td>\n",
       "      <td>1116.06</td>\n",
       "      <td>1176</td>\n",
       "      <td>404740</td>\n",
       "      <td>904230</td>\n",
       "      <td>622012</td>\n",
       "      <td>229790</td>\n",
       "      <td>405298</td>\n",
       "      <td>347188</td>\n",
       "      <td>286954</td>\n",
       "      <td>311560</td>\n",
       "      <td>433954</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  \\\n",
       "0   neg   76698     na  2130706438    280      0      0      0      0      0   \n",
       "1   neg   33058     na           0     na      0      0      0      0      0   \n",
       "2   neg   41040     na         228    100      0      0      0      0      0   \n",
       "3   neg      12      0          70     66      0     10      0      0      0   \n",
       "4   neg   60874     na        1368    458      0      0      0      0      0   \n",
       "\n",
       "  ag_003 ag_004   ag_005   ag_006   ag_007 ag_008 ag_009   ah_000 ai_000  \\\n",
       "0      0  37250  1432864  3664156  1007684  25896      0  2551696      0   \n",
       "1      0  18254   653294  1720800   516724  31642      0  1393352      0   \n",
       "2      0   1648   370592  1883374   292936  12016      0  1234132      0   \n",
       "3    318   2212     3232     1872        0      0      0     2668      0   \n",
       "4      0  43752  1966618  1800340   131646   4588      0  1974038      0   \n",
       "\n",
       "  aj_000 ak_000 al_000  am_0   an_000   ao_000   ap_000   aq_000 ar_000  \\\n",
       "0      0      0      0     0  4933296  3655166  1766008  1132040      0   \n",
       "1     68      0      0     0  2560898  2127150  1084598   338544      0   \n",
       "2      0      0      0     0  2371990  2173634   300796   153698      0   \n",
       "3      0      0    642  3894    10184     7554    10764     1014      0   \n",
       "4    226      0      0     0  3230626  2618878  1058136   551022      0   \n",
       "\n",
       "  as_000 at_000 au_000 av_000 ax_000 ay_000 ay_001 ay_002 ay_003 ay_004  \\\n",
       "0      0      0      0   1012    268      0      0      0      0      0   \n",
       "1      0      0      0      0      0      0      0      0      0      0   \n",
       "2      0      0      0    358    110      0      0      0      0      0   \n",
       "3      0      0      0     60      6      0      0      0      0      0   \n",
       "4      0      0      0   1788    642      0      0      0      0  42124   \n",
       "\n",
       "   ay_005   ay_006   ay_007   ay_008 ay_009 az_000 az_001 az_002 az_003  \\\n",
       "0  469014  4239660   703300   755876      0   5374   2108   4114  12348   \n",
       "1   71510   772720  1996924    99560      0   7336   7808  13776  13086   \n",
       "2       0   870456   239798  1450312      0   1620   1156   1228  34250   \n",
       "3       0        0     2038     5596      0     64      6      6    914   \n",
       "4  372236  2128914   819596   584074      0   1644    362    562    842   \n",
       "\n",
       "    az_004   az_005 az_006 az_007 az_008 az_009   ba_000   ba_001  ba_002  \\\n",
       "0   615248  5526276   2378      4      0      0  2328746  1022304  415432   \n",
       "1  1010074  1873902  14726      6      0      0  1378576   447166  199512   \n",
       "2  1811606   710672     34      0      0      0   790690   672026  332340   \n",
       "3       76     2478   2398   1692      0      0     6176      340     304   \n",
       "4    30194  3911734   1606      0      0      0  1348578  1035668  338762   \n",
       "\n",
       "   ba_003  ba_004  ba_005   ba_006  ba_007 ba_008 ba_009   bb_000 bc_000  \\\n",
       "0  287230  310246  681504  1118814    3574      0      0  6700214      0   \n",
       "1  154298  137280  138668   165908  229652  87082   4708  3646660     86   \n",
       "2  254892  189596  135758   103552   81666     46      0  2673338    128   \n",
       "3     102      74     406      216      16      0      0    21614      2   \n",
       "4  236540  182278  151778   163248  470800  19292      0  4289260    448   \n",
       "\n",
       "  bd_000 be_000 bf_000   bg_000 bh_000  bi_000  bj_000  bk_000  bl_000  \\\n",
       "0     10    108     50  2551696  97518  947550  799478  330760  353400   \n",
       "1    454    364    350  1393352  49028  688314  392208  341420  359780   \n",
       "2    202    576      4  1234132  28804  160176  139730  137160  130640   \n",
       "3     12      0      0     2668    184    7632    3090      na      na   \n",
       "4    556    642      2  1974038  86454  653692  399410  306780  282560   \n",
       "\n",
       "   bm_000  bn_000  bo_000 bp_000 bq_000 br_000  bs_000    bt_000   bu_000  \\\n",
       "0  299160  305200  283680     na     na     na  178540  76698.08  6700214   \n",
       "1  366560      na      na     na     na     na    6700  33057.51  3646660   \n",
       "2      na      na      na     na     na     na   28000  41040.08  2673338   \n",
       "3      na      na      na     na     na     na   10580     12.69    21614   \n",
       "4  274180      na      na     na     na     na  189000  60874.03  4289260   \n",
       "\n",
       "    bv_000   bx_000   by_000  bz_000  ca_000  cb_000   cc_000   cd_000  \\\n",
       "0  6700214  6599892    43566   68656   54064  638360  6167850  1209600   \n",
       "1  3646660  3582034  17733.0  260120  115626    6900  2942850  1209600   \n",
       "2  2673338  2678534  15439.0    7466   22436  248240  2560566  1209600   \n",
       "3    21614    21772       32      50    1994   21400     7710  1209600   \n",
       "4  4289260  4283332  24793.0   17052   61844  654700  3946944  1209600   \n",
       "\n",
       "   ce_000 cf_000 cg_000 ch_000      ci_000 cj_000     ck_000 cl_000 cm_000  \\\n",
       "0  246244      2     96      0     5245752      0  916567.68      6   1924   \n",
       "1       0     na     na     na  2291079.36      0  643536.96      0      0   \n",
       "2   63328      0    124      0  2322692.16      0  236099.52      0      0   \n",
       "3     302      2      6      0     2135.04      0    4525.44      2     16   \n",
       "4  135720      0    152      0   3565684.8      0  379111.68      0    746   \n",
       "\n",
       "  cn_000 cn_001 cn_002  cn_003   cn_004   cn_005   cn_006 cn_007 cn_008  \\\n",
       "0      0      0      0  118196  1309472  3247182  1381362  98822  11208   \n",
       "1      0      0     38   98644  1179502  1286736   336388  36294   5192   \n",
       "2      0      0      0   33276  1215280  1102798   196502  10260   2422   \n",
       "3      0     52   2544    1894     2170      822      152      0      0   \n",
       "4      0      0    356  378910  2497104   993000    64230  10482   2776   \n",
       "\n",
       "  cn_009 co_000 cp_000   cq_000 cr_000 cs_000 cs_001  cs_002  cs_003  cs_004  \\\n",
       "0   1608    220    240  6700214     na  10476   1226  267998  521832  428776   \n",
       "1     56     na      0  3646660     na   6160    796  164860  350066  272956   \n",
       "2     28      0      6  2673338     na   3584    500   56362  149726  100326   \n",
       "3      0      2      2    21614      0   1032      6      24     656     692   \n",
       "4     86    202    212  4289260     na   3942    520   80950  227322  186242   \n",
       "\n",
       "    cs_005   cs_006 cs_007 cs_008 cs_009 ct_000 cu_000   cv_000  cx_000  \\\n",
       "0  4015854   895240  26330    118      0    532    734  4122704   51288   \n",
       "1  1837600   301242   9148     22      0     na     na       na      na   \n",
       "2  1744838   488302  16682    246      0    230    292  2180528   29188   \n",
       "3     4836      388      0      0      0    138      8     1666      72   \n",
       "4  2288268  1137268  22228    204      0   1716   1664  3440288  215826   \n",
       "\n",
       "  cy_000  cz_000 da_000 db_000   dc_000 dd_000 de_000 df_000 dg_000 dh_000  \\\n",
       "0      0  532572      0     18  5330690   4732   1126      0      0      0   \n",
       "1     na      na     na     na       na   3312    522      0      0      0   \n",
       "2     22   20346      0      0  2341048   1494    152      0      0      0   \n",
       "3      0      12      0      0     2578     76     62      0      0      0   \n",
       "4      0    4262      0      0  3590004   2026    444      0      0      0   \n",
       "\n",
       "  di_000 dj_000 dk_000 dl_000 dm_000 dn_000 do_000 dp_000 dq_000 dr_000  \\\n",
       "0      0      0      0      0      0  62282  85908  32790      0      0   \n",
       "1      0      0      0      0      0  33736  36946   5936      0      0   \n",
       "2      0      0      0      0      0  13876  38182   8138      0      0   \n",
       "3      0      0      0      0      0    232      0      0   2014    370   \n",
       "4      0      0      0      0      0  44946  62648  11506      0      0   \n",
       "\n",
       "   ds_000 dt_000    du_000   dv_000 dx_000 dy_000 dz_000 ea_000    eb_000  \\\n",
       "0  202710  37928  14745580  1876644      0      0      0      0   2801180   \n",
       "1  103330  16254   4510080   868538      0      0      0      0   3477820   \n",
       "2   65772  10534    300240    48028      0      0      0      0   1040120   \n",
       "3      48     18     15740     1822  20174     44      0      0         0   \n",
       "4  149474  35154    457040    80482  98334  27588      0      0  21173050   \n",
       "\n",
       "     ec_00 ed_000  ee_000   ee_001   ee_002  ee_003  ee_004  ee_005  ee_006  \\\n",
       "0   2445.8   2712  965866  1706908  1240520  493384  721044  469792  339156   \n",
       "1  2211.76   2334  664504   824154   421400  178064  293306  245416  133654   \n",
       "2  1018.64   1020  262032   453378   277378  159812  423992  409564  320746   \n",
       "3     1.08     54    5670     1566      240      46      58      44      10   \n",
       "4  1116.06   1176  404740   904230   622012  229790  405298  347188  286954   \n",
       "\n",
       "   ee_007  ee_008 ee_009 ef_000 eg_000  \n",
       "0  157956   73224      0      0      0  \n",
       "1   81140   97576   1500      0      0  \n",
       "2  158022   95128    514      0      0  \n",
       "3       0       0      0      4     32  \n",
       "4  311560  433954   1218      0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929a260c-45eb-4f64-a73e-9aa781f2f137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:21.864382Z",
     "iopub.status.busy": "2024-07-20T13:55:21.864109Z",
     "iopub.status.idle": "2024-07-20T13:55:21.866710Z",
     "shell.execute_reply": "2024-07-20T13:55:21.866446Z",
     "shell.execute_reply.started": "2024-07-20T13:55:21.864370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 171)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fcb474-fae2-41dd-90ce-33f7745aa002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:22.127864Z",
     "iopub.status.busy": "2024-07-20T13:55:22.127579Z",
     "iopub.status.idle": "2024-07-20T13:55:22.538318Z",
     "shell.execute_reply": "2024-07-20T13:55:22.538001Z",
     "shell.execute_reply.started": "2024-07-20T13:55:22.127851Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df[i] = df[i].replace('na', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e118b3-23cb-4445-bd13-82cec15953c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:22.545483Z",
     "iopub.status.busy": "2024-07-20T13:55:22.545342Z",
     "iopub.status.idle": "2024-07-20T13:55:22.858637Z",
     "shell.execute_reply": "2024-07-20T13:55:22.858302Z",
     "shell.execute_reply.started": "2024-07-20T13:55:22.545468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 143)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retirando colunas que possuem mais de 10% de valores nulos\n",
    "\n",
    "missing_proportion = df.isnull().mean().sort_values()\n",
    "cols_to_drop = missing_proportion[missing_proportion > 0.10].index\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55f93d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:23.185071Z",
     "iopub.status.busy": "2024-07-20T13:55:23.184903Z",
     "iopub.status.idle": "2024-07-20T13:55:23.958921Z",
     "shell.execute_reply": "2024-07-20T13:55:23.958591Z",
     "shell.execute_reply.started": "2024-07-20T13:55:23.185058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 22, 29, 48, 50, 54, 57, 69, 73, 135, 155, 160, 182, 222, 257, 334, 337, 403, 419, 448, 467, 618, 774, 895, 920, 987, 1033, 1056, 1099, 1272, 1314, 1531, 1768, 1894, 2062, 2216, 2316, 2423, 2808, 3056, 3344, 3512, 3566, 3861, 3877, 3919, 4056, 5356, 5420, 5619, 6312, 6564, 6683, 6798, 7379, 7618, 7880, 8111, 9018, 9065, 9185, 9298, 9725, 10118, 10583, 11569, 11817, 12307, 12835, 14736, 14790, 15911, 16548, 17301, 18933, 19697, 20515, 21099, 21531, 21997, 22076, 22095, 22628, 23072, 24214, 25167, 25241, 26313, 27832, 28278, 28319, 28496, 29282, 29594, 30140, 30171, 30470, 30674, 31712, 31796, 32126, 32427, 32915, 34078, 34111, 34489, 34559, 34629, 34922, 35189, 35194, 35959, 36289, 37823, 38246, 38360, 38555, 38652, 40165, 40616, 40798, 40917, 41816, 42087, 42099, 42559, 43569, 43572, 43860, 44602, 44673, 44841, 45044, 45481, 45965, 47724, 48205, 49347, 49348, 49350, 49393, 52849]\n"
     ]
    }
   ],
   "source": [
    "## Valores nicos das variveis do dataset\n",
    "\n",
    "unique_values = []\n",
    "\n",
    "for col in df.columns:\n",
    "    valores_unicos = len(df[col].astype(str).value_counts())\n",
    "    unique_values.append(valores_unicos)\n",
    "\n",
    "unique_values.sort()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab730fb-fc76-4d05-86ad-235d84266e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:23.965031Z",
     "iopub.status.busy": "2024-07-20T13:55:23.964938Z",
     "iopub.status.idle": "2024-07-20T13:55:24.177124Z",
     "shell.execute_reply": "2024-07-20T13:55:24.176817Z",
     "shell.execute_reply.started": "2024-07-20T13:55:23.965022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variavel categorica class\n",
      "Variavel categorica cd_000\n"
     ]
    }
   ],
   "source": [
    "## Supondo que variveis com menos de 10 valores distintos so categricas\n",
    "\n",
    "categorical_columns = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if len(df[i].unique()) < 10:\n",
    "        categorical_columns.append(i)\n",
    "        print(f\"Variavel categorica {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ae7b5d-beee-4a66-823f-72703f70f822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:24.232934Z",
     "iopub.status.busy": "2024-07-20T13:55:24.232547Z",
     "iopub.status.idle": "2024-07-20T13:55:24.316349Z",
     "shell.execute_reply": "2024-07-20T13:55:24.316063Z",
     "shell.execute_reply.started": "2024-07-20T13:55:24.232917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Proporo de negativos e positivos'}, xlabel='class'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"class\"].value_counts(normalize=True).plot(kind=\"bar\", title=\"Proporo de negativos e positivos\", color=[\"green\",\"red\"])\n",
    "# Varivel alvo desbalanceada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f97bedd-8520-4e17-972e-9b5306f75d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:24.633178Z",
     "iopub.status.busy": "2024-07-20T13:55:24.632902Z",
     "iopub.status.idle": "2024-07-20T13:55:24.639185Z",
     "shell.execute_reply": "2024-07-20T13:55:24.638807Z",
     "shell.execute_reply.started": "2024-07-20T13:55:24.633166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "neg    59000\n",
       "pos     1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].value_counts() # Valores absolutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be421023-b904-4d7d-8629-5e4faac69600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:24.888849Z",
     "iopub.status.busy": "2024-07-20T13:55:24.888677Z",
     "iopub.status.idle": "2024-07-20T13:55:24.895124Z",
     "shell.execute_reply": "2024-07-20T13:55:24.894736Z",
     "shell.execute_reply.started": "2024-07-20T13:55:24.888836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cd_000\n",
       "1209600    59324\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cd_000\"].value_counts() # Varivel possivelmente categrica com apenas uma classe. Passvel de excluso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d23da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:25.160639Z",
     "iopub.status.busy": "2024-07-20T13:55:25.160437Z",
     "iopub.status.idle": "2024-07-20T13:55:25.163189Z",
     "shell.execute_reply": "2024-07-20T13:55:25.162894Z",
     "shell.execute_reply.started": "2024-07-20T13:55:25.160626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 143)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variveis categricas possivelmente inexistentes\n",
    "\n",
    "# df = df.drop(columns=\"cd_000\")\n",
    "# categorical_columns.remove(\"cd_000\") \n",
    "\n",
    "### EXEMPLO DE VARIAVEL QUE VAI SER ELIMIADA PELO VARIANCETHRESHOLD\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32e9968-fc1e-4c6f-abb5-9fda7aa5ba5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:25.576196Z",
     "iopub.status.busy": "2024-07-20T13:55:25.576030Z",
     "iopub.status.idle": "2024-07-20T13:55:25.578296Z",
     "shell.execute_reply": "2024-07-20T13:55:25.577982Z",
     "shell.execute_reply.started": "2024-07-20T13:55:25.576185Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rotulando o resto das variveis como numricas\n",
    "\n",
    "numerical_cols = [col for col in df.columns if col not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68799c75-5134-49d6-8d68-3c76f3cbf6be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:26.191913Z",
     "iopub.status.busy": "2024-07-20T13:55:26.191632Z",
     "iopub.status.idle": "2024-07-20T13:55:26.194582Z",
     "shell.execute_reply": "2024-07-20T13:55:26.194184Z",
     "shell.execute_reply.started": "2024-07-20T13:55:26.191900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class', 'cd_000']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed0e2ea-da0d-46cf-9ec9-e6f06b764b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:26.480115Z",
     "iopub.status.busy": "2024-07-20T13:55:26.479838Z",
     "iopub.status.idle": "2024-07-20T13:55:26.944900Z",
     "shell.execute_reply": "2024-07-20T13:55:26.944557Z",
     "shell.execute_reply.started": "2024-07-20T13:55:26.480101Z"
    }
   },
   "outputs": [],
   "source": [
    "## Transformando as variveis numericas para float\n",
    "# As variveis categricas ja esto no tipo esperado\n",
    "\n",
    "for var in numerical_cols:\n",
    "    df[var] = df[var].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "328dadc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:26.969991Z",
     "iopub.status.busy": "2024-07-20T13:55:26.969770Z",
     "iopub.status.idle": "2024-07-20T13:55:27.297873Z",
     "shell.execute_reply": "2024-07-20T13:55:27.297593Z",
     "shell.execute_reply.started": "2024-07-20T13:55:26.969980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>ag_004</th>\n",
       "      <th>ag_005</th>\n",
       "      <th>ag_006</th>\n",
       "      <th>ag_007</th>\n",
       "      <th>ag_008</th>\n",
       "      <th>ag_009</th>\n",
       "      <th>ah_000</th>\n",
       "      <th>ai_000</th>\n",
       "      <th>aj_000</th>\n",
       "      <th>ak_000</th>\n",
       "      <th>al_000</th>\n",
       "      <th>am_0</th>\n",
       "      <th>an_000</th>\n",
       "      <th>ao_000</th>\n",
       "      <th>ap_000</th>\n",
       "      <th>aq_000</th>\n",
       "      <th>ar_000</th>\n",
       "      <th>as_000</th>\n",
       "      <th>at_000</th>\n",
       "      <th>au_000</th>\n",
       "      <th>av_000</th>\n",
       "      <th>ax_000</th>\n",
       "      <th>ay_000</th>\n",
       "      <th>ay_001</th>\n",
       "      <th>ay_002</th>\n",
       "      <th>ay_003</th>\n",
       "      <th>ay_004</th>\n",
       "      <th>ay_005</th>\n",
       "      <th>ay_006</th>\n",
       "      <th>ay_007</th>\n",
       "      <th>ay_008</th>\n",
       "      <th>ay_009</th>\n",
       "      <th>az_000</th>\n",
       "      <th>az_001</th>\n",
       "      <th>az_002</th>\n",
       "      <th>az_003</th>\n",
       "      <th>az_004</th>\n",
       "      <th>az_005</th>\n",
       "      <th>az_006</th>\n",
       "      <th>az_007</th>\n",
       "      <th>az_008</th>\n",
       "      <th>az_009</th>\n",
       "      <th>ba_000</th>\n",
       "      <th>ba_001</th>\n",
       "      <th>ba_002</th>\n",
       "      <th>ba_003</th>\n",
       "      <th>ba_004</th>\n",
       "      <th>ba_005</th>\n",
       "      <th>ba_006</th>\n",
       "      <th>ba_007</th>\n",
       "      <th>ba_008</th>\n",
       "      <th>ba_009</th>\n",
       "      <th>bb_000</th>\n",
       "      <th>bc_000</th>\n",
       "      <th>bd_000</th>\n",
       "      <th>be_000</th>\n",
       "      <th>bf_000</th>\n",
       "      <th>bg_000</th>\n",
       "      <th>bh_000</th>\n",
       "      <th>bi_000</th>\n",
       "      <th>bj_000</th>\n",
       "      <th>bs_000</th>\n",
       "      <th>bt_000</th>\n",
       "      <th>bu_000</th>\n",
       "      <th>bv_000</th>\n",
       "      <th>bx_000</th>\n",
       "      <th>by_000</th>\n",
       "      <th>bz_000</th>\n",
       "      <th>ca_000</th>\n",
       "      <th>cb_000</th>\n",
       "      <th>cc_000</th>\n",
       "      <th>ce_000</th>\n",
       "      <th>ci_000</th>\n",
       "      <th>cj_000</th>\n",
       "      <th>ck_000</th>\n",
       "      <th>cn_000</th>\n",
       "      <th>cn_001</th>\n",
       "      <th>cn_002</th>\n",
       "      <th>cn_003</th>\n",
       "      <th>cn_004</th>\n",
       "      <th>cn_005</th>\n",
       "      <th>cn_006</th>\n",
       "      <th>cn_007</th>\n",
       "      <th>cn_008</th>\n",
       "      <th>cn_009</th>\n",
       "      <th>cp_000</th>\n",
       "      <th>cq_000</th>\n",
       "      <th>cs_000</th>\n",
       "      <th>cs_001</th>\n",
       "      <th>cs_002</th>\n",
       "      <th>cs_003</th>\n",
       "      <th>cs_004</th>\n",
       "      <th>cs_005</th>\n",
       "      <th>cs_006</th>\n",
       "      <th>cs_007</th>\n",
       "      <th>cs_008</th>\n",
       "      <th>cs_009</th>\n",
       "      <th>dd_000</th>\n",
       "      <th>de_000</th>\n",
       "      <th>df_000</th>\n",
       "      <th>dg_000</th>\n",
       "      <th>dh_000</th>\n",
       "      <th>di_000</th>\n",
       "      <th>dj_000</th>\n",
       "      <th>dk_000</th>\n",
       "      <th>dl_000</th>\n",
       "      <th>dm_000</th>\n",
       "      <th>dn_000</th>\n",
       "      <th>do_000</th>\n",
       "      <th>dp_000</th>\n",
       "      <th>dq_000</th>\n",
       "      <th>dr_000</th>\n",
       "      <th>ds_000</th>\n",
       "      <th>dt_000</th>\n",
       "      <th>du_000</th>\n",
       "      <th>dv_000</th>\n",
       "      <th>dx_000</th>\n",
       "      <th>dy_000</th>\n",
       "      <th>dz_000</th>\n",
       "      <th>ea_000</th>\n",
       "      <th>eb_000</th>\n",
       "      <th>ee_000</th>\n",
       "      <th>ee_001</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000e+04</td>\n",
       "      <td>5.666500e+04</td>\n",
       "      <td>57500.000000</td>\n",
       "      <td>57500.000000</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.935500e+04</td>\n",
       "      <td>5.937100e+04</td>\n",
       "      <td>5.937100e+04</td>\n",
       "      <td>5.560000e+04</td>\n",
       "      <td>5.935800e+04</td>\n",
       "      <td>5.937100e+04</td>\n",
       "      <td>5.935800e+04</td>\n",
       "      <td>5.941100e+04</td>\n",
       "      <td>5.935800e+04</td>\n",
       "      <td>5.941100e+04</td>\n",
       "      <td>57277.000000</td>\n",
       "      <td>5.937100e+04</td>\n",
       "      <td>5.937100e+04</td>\n",
       "      <td>5.937100e+04</td>\n",
       "      <td>57500.000000</td>\n",
       "      <td>57499.000000</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>59329.000000</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.931200e+04</td>\n",
       "      <td>5.935500e+04</td>\n",
       "      <td>57275.000000</td>\n",
       "      <td>57273.000000</td>\n",
       "      <td>57497.000000</td>\n",
       "      <td>57500.000000</td>\n",
       "      <td>5.935800e+04</td>\n",
       "      <td>5.935800e+04</td>\n",
       "      <td>5.941100e+04</td>\n",
       "      <td>5.941100e+04</td>\n",
       "      <td>5.927400e+04</td>\n",
       "      <td>5.983300e+04</td>\n",
       "      <td>5.930900e+04</td>\n",
       "      <td>5.930900e+04</td>\n",
       "      <td>5.674300e+04</td>\n",
       "      <td>5.952700e+04</td>\n",
       "      <td>5.727700e+04</td>\n",
       "      <td>55644.000000</td>\n",
       "      <td>5.927400e+04</td>\n",
       "      <td>5.674500e+04</td>\n",
       "      <td>5.749800e+04</td>\n",
       "      <td>5.966200e+04</td>\n",
       "      <td>5.966200e+04</td>\n",
       "      <td>5.966200e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>5.931300e+04</td>\n",
       "      <td>57276.000000</td>\n",
       "      <td>5.930900e+04</td>\n",
       "      <td>59331.000000</td>\n",
       "      <td>59331.000000</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>5.933100e+04</td>\n",
       "      <td>57497.000000</td>\n",
       "      <td>57276.000000</td>\n",
       "      <td>5.599200e+04</td>\n",
       "      <td>5.599200e+04</td>\n",
       "      <td>5.599200e+04</td>\n",
       "      <td>5.599400e+04</td>\n",
       "      <td>55993.000000</td>\n",
       "      <td>5.599300e+04</td>\n",
       "      <td>5.599200e+04</td>\n",
       "      <td>5.599100e+04</td>\n",
       "      <td>5.930900e+04</td>\n",
       "      <td>5.727600e+04</td>\n",
       "      <td>57274.000000</td>\n",
       "      <td>5.727400e+04</td>\n",
       "      <td>5.727400e+04</td>\n",
       "      <td>5.727300e+04</td>\n",
       "      <td>57273.000000</td>\n",
       "      <td>5.727400e+04</td>\n",
       "      <td>5.727400e+04</td>\n",
       "      <td>5.727700e+04</td>\n",
       "      <td>5.727600e+04</td>\n",
       "      <td>57277.000000</td>\n",
       "      <td>57277.000000</td>\n",
       "      <td>5.599300e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>5.932900e+04</td>\n",
       "      <td>57276.000000</td>\n",
       "      <td>57277.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.933650e+04</td>\n",
       "      <td>3.560143e+08</td>\n",
       "      <td>6.819130</td>\n",
       "      <td>11.006817</td>\n",
       "      <td>2.216364e+02</td>\n",
       "      <td>9.757223e+02</td>\n",
       "      <td>8.606015e+03</td>\n",
       "      <td>8.859128e+04</td>\n",
       "      <td>4.370966e+05</td>\n",
       "      <td>1.108374e+06</td>\n",
       "      <td>1.657818e+06</td>\n",
       "      <td>4.993098e+05</td>\n",
       "      <td>3.556989e+04</td>\n",
       "      <td>5.114753e+03</td>\n",
       "      <td>1.809931e+06</td>\n",
       "      <td>9.016965e+03</td>\n",
       "      <td>1.143675e+03</td>\n",
       "      <td>9.794900e+02</td>\n",
       "      <td>5.913048e+04</td>\n",
       "      <td>9.328133e+04</td>\n",
       "      <td>3.461037e+06</td>\n",
       "      <td>3.002440e+06</td>\n",
       "      <td>1.004160e+06</td>\n",
       "      <td>4.424045e+05</td>\n",
       "      <td>0.496918</td>\n",
       "      <td>1.267365e+02</td>\n",
       "      <td>5.072046e+03</td>\n",
       "      <td>2.305804e+02</td>\n",
       "      <td>1117.825913</td>\n",
       "      <td>374.327380</td>\n",
       "      <td>1.221165e+04</td>\n",
       "      <td>1.019012e+04</td>\n",
       "      <td>1.097500e+04</td>\n",
       "      <td>7.225784e+03</td>\n",
       "      <td>1.056600e+04</td>\n",
       "      <td>1.119791e+05</td>\n",
       "      <td>1.078551e+06</td>\n",
       "      <td>1.546032e+06</td>\n",
       "      <td>1.051123e+06</td>\n",
       "      <td>1.162622e+03</td>\n",
       "      <td>7.849608e+03</td>\n",
       "      <td>4.420992e+03</td>\n",
       "      <td>8.066082e+03</td>\n",
       "      <td>8.724082e+04</td>\n",
       "      <td>1.476897e+06</td>\n",
       "      <td>2.135584e+06</td>\n",
       "      <td>1.018943e+05</td>\n",
       "      <td>1.737782e+04</td>\n",
       "      <td>6.617861e+02</td>\n",
       "      <td>42.073455</td>\n",
       "      <td>1.399652e+06</td>\n",
       "      <td>8.941175e+05</td>\n",
       "      <td>4.130969e+05</td>\n",
       "      <td>2.740070e+05</td>\n",
       "      <td>2.048756e+05</td>\n",
       "      <td>1.889412e+05</td>\n",
       "      <td>2.106288e+05</td>\n",
       "      <td>1.857874e+05</td>\n",
       "      <td>3.588284e+04</td>\n",
       "      <td>3.576672e+04</td>\n",
       "      <td>4.526177e+06</td>\n",
       "      <td>569.526565</td>\n",
       "      <td>921.775461</td>\n",
       "      <td>1372.646086</td>\n",
       "      <td>74.878261</td>\n",
       "      <td>1.809431e+06</td>\n",
       "      <td>5.794308e+04</td>\n",
       "      <td>4.922076e+05</td>\n",
       "      <td>5.100892e+05</td>\n",
       "      <td>8.036055e+04</td>\n",
       "      <td>5.941650e+04</td>\n",
       "      <td>4.515325e+06</td>\n",
       "      <td>4.515325e+06</td>\n",
       "      <td>4.112218e+06</td>\n",
       "      <td>2.202893e+04</td>\n",
       "      <td>1.019608e+05</td>\n",
       "      <td>39168.817123</td>\n",
       "      <td>4.056381e+05</td>\n",
       "      <td>3.803444e+06</td>\n",
       "      <td>6.434356e+04</td>\n",
       "      <td>3.481204e+06</td>\n",
       "      <td>1.028419e+05</td>\n",
       "      <td>7.143427e+05</td>\n",
       "      <td>2.336674e+03</td>\n",
       "      <td>2.195149e+04</td>\n",
       "      <td>1.610509e+05</td>\n",
       "      <td>5.314780e+05</td>\n",
       "      <td>1.282835e+06</td>\n",
       "      <td>1.341059e+06</td>\n",
       "      <td>4.105641e+05</td>\n",
       "      <td>6.442513e+04</td>\n",
       "      <td>1.922679e+04</td>\n",
       "      <td>7.820467e+03</td>\n",
       "      <td>570.404288</td>\n",
       "      <td>4.515325e+06</td>\n",
       "      <td>5479.857073</td>\n",
       "      <td>788.425545</td>\n",
       "      <td>2.388106e+05</td>\n",
       "      <td>3.553731e+05</td>\n",
       "      <td>4.442283e+05</td>\n",
       "      <td>2.235387e+06</td>\n",
       "      <td>5.457742e+05</td>\n",
       "      <td>1.477142e+04</td>\n",
       "      <td>2.117473e+02</td>\n",
       "      <td>7.791978e+02</td>\n",
       "      <td>3123.961911</td>\n",
       "      <td>375.147112</td>\n",
       "      <td>2.718638e+03</td>\n",
       "      <td>5.609957e+03</td>\n",
       "      <td>4.707073e+03</td>\n",
       "      <td>3.724824e+04</td>\n",
       "      <td>39.938564</td>\n",
       "      <td>1.861313e+03</td>\n",
       "      <td>2.854177e+04</td>\n",
       "      <td>7.923228e+03</td>\n",
       "      <td>3.374545e+04</td>\n",
       "      <td>2.850785e+04</td>\n",
       "      <td>6958.652722</td>\n",
       "      <td>4.529375e+06</td>\n",
       "      <td>2.037598e+05</td>\n",
       "      <td>8.965500e+04</td>\n",
       "      <td>15403.354670</td>\n",
       "      <td>4.058712e+06</td>\n",
       "      <td>5.938350e+05</td>\n",
       "      <td>7.912085e+05</td>\n",
       "      <td>7.780350e+03</td>\n",
       "      <td>0.215759</td>\n",
       "      <td>1.567750</td>\n",
       "      <td>9.717093e+06</td>\n",
       "      <td>7.334042e+05</td>\n",
       "      <td>7.838746e+05</td>\n",
       "      <td>4.454897e+05</td>\n",
       "      <td>2.111264e+05</td>\n",
       "      <td>4.457343e+05</td>\n",
       "      <td>3.939462e+05</td>\n",
       "      <td>3.330582e+05</td>\n",
       "      <td>3.462714e+05</td>\n",
       "      <td>1.387300e+05</td>\n",
       "      <td>8.388915e+03</td>\n",
       "      <td>0.090579</td>\n",
       "      <td>0.212756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.454301e+05</td>\n",
       "      <td>7.948749e+08</td>\n",
       "      <td>161.543373</td>\n",
       "      <td>209.792592</td>\n",
       "      <td>2.047846e+04</td>\n",
       "      <td>3.420053e+04</td>\n",
       "      <td>1.503220e+05</td>\n",
       "      <td>7.617312e+05</td>\n",
       "      <td>2.374282e+06</td>\n",
       "      <td>3.262607e+06</td>\n",
       "      <td>3.909384e+06</td>\n",
       "      <td>1.422765e+06</td>\n",
       "      <td>2.201524e+05</td>\n",
       "      <td>1.696582e+05</td>\n",
       "      <td>4.185740e+06</td>\n",
       "      <td>1.632778e+05</td>\n",
       "      <td>5.035971e+04</td>\n",
       "      <td>7.583162e+04</td>\n",
       "      <td>5.394658e+05</td>\n",
       "      <td>8.494694e+05</td>\n",
       "      <td>7.790350e+06</td>\n",
       "      <td>6.819518e+06</td>\n",
       "      <td>3.088457e+06</td>\n",
       "      <td>1.262469e+06</td>\n",
       "      <td>5.511653</td>\n",
       "      <td>1.101004e+04</td>\n",
       "      <td>1.196159e+05</td>\n",
       "      <td>1.579952e+04</td>\n",
       "      <td>6598.611557</td>\n",
       "      <td>1482.711621</td>\n",
       "      <td>4.544963e+05</td>\n",
       "      <td>5.352707e+05</td>\n",
       "      <td>4.283370e+05</td>\n",
       "      <td>2.064679e+05</td>\n",
       "      <td>3.546258e+05</td>\n",
       "      <td>1.394585e+06</td>\n",
       "      <td>3.278941e+06</td>\n",
       "      <td>5.106177e+06</td>\n",
       "      <td>3.991050e+06</td>\n",
       "      <td>9.796080e+04</td>\n",
       "      <td>7.363676e+04</td>\n",
       "      <td>3.399539e+04</td>\n",
       "      <td>1.065997e+05</td>\n",
       "      <td>6.532562e+05</td>\n",
       "      <td>4.184087e+06</td>\n",
       "      <td>6.460221e+06</td>\n",
       "      <td>8.997699e+05</td>\n",
       "      <td>2.804446e+05</td>\n",
       "      <td>1.492475e+04</td>\n",
       "      <td>3256.569903</td>\n",
       "      <td>3.777091e+06</td>\n",
       "      <td>2.346002e+06</td>\n",
       "      <td>1.196083e+06</td>\n",
       "      <td>7.485388e+05</td>\n",
       "      <td>5.390584e+05</td>\n",
       "      <td>5.092672e+05</td>\n",
       "      <td>6.370815e+05</td>\n",
       "      <td>5.251506e+05</td>\n",
       "      <td>2.436657e+05</td>\n",
       "      <td>3.353276e+05</td>\n",
       "      <td>1.088674e+07</td>\n",
       "      <td>4045.937032</td>\n",
       "      <td>4833.065431</td>\n",
       "      <td>9249.936515</td>\n",
       "      <td>546.789611</td>\n",
       "      <td>4.180200e+06</td>\n",
       "      <td>1.522093e+05</td>\n",
       "      <td>1.485185e+06</td>\n",
       "      <td>1.820105e+06</td>\n",
       "      <td>8.451276e+04</td>\n",
       "      <td>1.454464e+05</td>\n",
       "      <td>1.085990e+07</td>\n",
       "      <td>1.085990e+07</td>\n",
       "      <td>1.035154e+07</td>\n",
       "      <td>5.399282e+04</td>\n",
       "      <td>6.289129e+05</td>\n",
       "      <td>36748.302141</td>\n",
       "      <td>3.693868e+05</td>\n",
       "      <td>9.625672e+06</td>\n",
       "      <td>1.428469e+05</td>\n",
       "      <td>8.355997e+06</td>\n",
       "      <td>1.135174e+06</td>\n",
       "      <td>2.180714e+06</td>\n",
       "      <td>6.118383e+04</td>\n",
       "      <td>2.457994e+05</td>\n",
       "      <td>1.073893e+06</td>\n",
       "      <td>2.216301e+06</td>\n",
       "      <td>3.357254e+06</td>\n",
       "      <td>3.144659e+06</td>\n",
       "      <td>1.294208e+06</td>\n",
       "      <td>4.064541e+05</td>\n",
       "      <td>1.862697e+05</td>\n",
       "      <td>2.664928e+05</td>\n",
       "      <td>7773.264479</td>\n",
       "      <td>1.085990e+07</td>\n",
       "      <td>10313.977722</td>\n",
       "      <td>2886.331336</td>\n",
       "      <td>1.215538e+06</td>\n",
       "      <td>1.110964e+06</td>\n",
       "      <td>2.073937e+06</td>\n",
       "      <td>5.613545e+06</td>\n",
       "      <td>1.168199e+06</td>\n",
       "      <td>8.005122e+04</td>\n",
       "      <td>1.015337e+04</td>\n",
       "      <td>1.843585e+05</td>\n",
       "      <td>9516.675102</td>\n",
       "      <td>1689.062059</td>\n",
       "      <td>1.373331e+05</td>\n",
       "      <td>2.085649e+05</td>\n",
       "      <td>5.602799e+05</td>\n",
       "      <td>4.254382e+05</td>\n",
       "      <td>4533.142182</td>\n",
       "      <td>6.659786e+04</td>\n",
       "      <td>1.095662e+06</td>\n",
       "      <td>2.775356e+05</td>\n",
       "      <td>9.733719e+04</td>\n",
       "      <td>6.125476e+04</td>\n",
       "      <td>13955.451781</td>\n",
       "      <td>9.748478e+07</td>\n",
       "      <td>1.366065e+06</td>\n",
       "      <td>2.082016e+05</td>\n",
       "      <td>33801.022975</td>\n",
       "      <td>1.156777e+07</td>\n",
       "      <td>2.082998e+06</td>\n",
       "      <td>4.151033e+06</td>\n",
       "      <td>5.924449e+04</td>\n",
       "      <td>10.821035</td>\n",
       "      <td>53.528722</td>\n",
       "      <td>4.274675e+07</td>\n",
       "      <td>2.416166e+06</td>\n",
       "      <td>2.570111e+06</td>\n",
       "      <td>1.155540e+06</td>\n",
       "      <td>5.433188e+05</td>\n",
       "      <td>1.168314e+06</td>\n",
       "      <td>1.121044e+06</td>\n",
       "      <td>1.069160e+06</td>\n",
       "      <td>1.728056e+06</td>\n",
       "      <td>4.495100e+05</td>\n",
       "      <td>4.747043e+04</td>\n",
       "      <td>4.368855</td>\n",
       "      <td>8.830641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.720000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.340000e+02</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.080000e+02</td>\n",
       "      <td>1.383400e+04</td>\n",
       "      <td>1.060800e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.973300e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.323850e+04</td>\n",
       "      <td>6.558500e+04</td>\n",
       "      <td>2.518900e+04</td>\n",
       "      <td>4.161000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.118000e+03</td>\n",
       "      <td>7.524000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.028000e+03</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>2.940000e+02</td>\n",
       "      <td>1.542000e+03</td>\n",
       "      <td>3.863800e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.347850e+04</td>\n",
       "      <td>1.483350e+04</td>\n",
       "      <td>5.207500e+03</td>\n",
       "      <td>1.842000e+03</td>\n",
       "      <td>6.120000e+02</td>\n",
       "      <td>3.740000e+02</td>\n",
       "      <td>3.520000e+02</td>\n",
       "      <td>7.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.056010e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.974300e+04</td>\n",
       "      <td>8.520000e+02</td>\n",
       "      <td>1.594700e+04</td>\n",
       "      <td>8.522000e+03</td>\n",
       "      <td>1.730000e+04</td>\n",
       "      <td>8.628300e+02</td>\n",
       "      <td>1.054440e+05</td>\n",
       "      <td>1.054440e+05</td>\n",
       "      <td>8.964900e+04</td>\n",
       "      <td>2.160000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6886.000000</td>\n",
       "      <td>7.712500e+04</td>\n",
       "      <td>6.241600e+04</td>\n",
       "      <td>2.660000e+02</td>\n",
       "      <td>4.824984e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.458672e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.622000e+03</td>\n",
       "      <td>1.911400e+04</td>\n",
       "      <td>5.028000e+03</td>\n",
       "      <td>6.260000e+02</td>\n",
       "      <td>6.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.054440e+05</td>\n",
       "      <td>1232.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.220000e+02</td>\n",
       "      <td>3.079000e+03</td>\n",
       "      <td>2.726000e+03</td>\n",
       "      <td>1.918600e+04</td>\n",
       "      <td>1.336000e+04</td>\n",
       "      <td>1.204000e+03</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.600000e+02</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.840000e+02</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>5.380000e+03</td>\n",
       "      <td>7.420000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.569800e+04</td>\n",
       "      <td>8.536000e+03</td>\n",
       "      <td>2.936000e+03</td>\n",
       "      <td>1.166000e+03</td>\n",
       "      <td>2.700000e+03</td>\n",
       "      <td>3.584000e+03</td>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.077600e+04</td>\n",
       "      <td>1.520000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.672000e+03</td>\n",
       "      <td>1.760200e+05</td>\n",
       "      <td>9.303360e+05</td>\n",
       "      <td>1.192040e+05</td>\n",
       "      <td>1.786000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.002420e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.918629e+06</td>\n",
       "      <td>1.643556e+06</td>\n",
       "      <td>3.572810e+05</td>\n",
       "      <td>1.787920e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.682020e+05</td>\n",
       "      <td>3.486220e+05</td>\n",
       "      <td>9.481200e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.098000e+03</td>\n",
       "      <td>6.360000e+02</td>\n",
       "      <td>1.016000e+03</td>\n",
       "      <td>3.570000e+03</td>\n",
       "      <td>8.161400e+04</td>\n",
       "      <td>5.270340e+05</td>\n",
       "      <td>2.920000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.794860e+05</td>\n",
       "      <td>4.436410e+05</td>\n",
       "      <td>1.860460e+05</td>\n",
       "      <td>1.341490e+05</td>\n",
       "      <td>1.018740e+05</td>\n",
       "      <td>8.399200e+04</td>\n",
       "      <td>7.011600e+04</td>\n",
       "      <td>4.470000e+03</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.360728e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.002718e+06</td>\n",
       "      <td>2.635200e+04</td>\n",
       "      <td>1.798420e+05</td>\n",
       "      <td>1.544040e+05</td>\n",
       "      <td>5.054000e+04</td>\n",
       "      <td>3.083986e+04</td>\n",
       "      <td>2.359656e+06</td>\n",
       "      <td>2.359656e+06</td>\n",
       "      <td>2.258824e+06</td>\n",
       "      <td>1.262800e+04</td>\n",
       "      <td>1.036000e+03</td>\n",
       "      <td>25436.000000</td>\n",
       "      <td>2.789900e+05</td>\n",
       "      <td>2.108912e+06</td>\n",
       "      <td>3.409000e+03</td>\n",
       "      <td>1.858641e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.502672e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.499400e+04</td>\n",
       "      <td>5.184620e+05</td>\n",
       "      <td>7.035240e+05</td>\n",
       "      <td>9.626600e+04</td>\n",
       "      <td>9.976000e+03</td>\n",
       "      <td>1.852000e+03</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.359656e+06</td>\n",
       "      <td>3192.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>2.057000e+04</td>\n",
       "      <td>1.217800e+05</td>\n",
       "      <td>9.108000e+04</td>\n",
       "      <td>1.220860e+06</td>\n",
       "      <td>2.407440e+05</td>\n",
       "      <td>6.104000e+03</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.433000e+04</td>\n",
       "      <td>1.037700e+04</td>\n",
       "      <td>2532.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.794000e+04</td>\n",
       "      <td>8316.000000</td>\n",
       "      <td>1.854000e+05</td>\n",
       "      <td>3.059200e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.221100e+05</td>\n",
       "      <td>2.607040e+05</td>\n",
       "      <td>3.469400e+05</td>\n",
       "      <td>2.337960e+05</td>\n",
       "      <td>1.120860e+05</td>\n",
       "      <td>2.215180e+05</td>\n",
       "      <td>1.899880e+05</td>\n",
       "      <td>9.243200e+04</td>\n",
       "      <td>4.109800e+04</td>\n",
       "      <td>3.812000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.866800e+04</td>\n",
       "      <td>9.640000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.952200e+04</td>\n",
       "      <td>9.139640e+05</td>\n",
       "      <td>1.886608e+06</td>\n",
       "      <td>5.888200e+05</td>\n",
       "      <td>2.669000e+04</td>\n",
       "      <td>3.640000e+02</td>\n",
       "      <td>1.601366e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.204000e+03</td>\n",
       "      <td>2.364000e+03</td>\n",
       "      <td>3.128416e+06</td>\n",
       "      <td>2.675796e+06</td>\n",
       "      <td>7.246605e+05</td>\n",
       "      <td>3.769000e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.013200e+04</td>\n",
       "      <td>1.270244e+06</td>\n",
       "      <td>1.337364e+06</td>\n",
       "      <td>6.118160e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.150000e+03</td>\n",
       "      <td>2.016000e+03</td>\n",
       "      <td>3.136000e+03</td>\n",
       "      <td>4.301400e+04</td>\n",
       "      <td>1.774410e+06</td>\n",
       "      <td>1.796172e+06</td>\n",
       "      <td>4.218000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.276368e+06</td>\n",
       "      <td>8.110005e+05</td>\n",
       "      <td>3.404085e+05</td>\n",
       "      <td>2.444490e+05</td>\n",
       "      <td>1.971620e+05</td>\n",
       "      <td>1.846740e+05</td>\n",
       "      <td>2.049590e+05</td>\n",
       "      <td>2.074710e+05</td>\n",
       "      <td>1.820000e+03</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>3.868370e+06</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.602766e+06</td>\n",
       "      <td>4.908550e+04</td>\n",
       "      <td>3.796100e+05</td>\n",
       "      <td>3.336080e+05</td>\n",
       "      <td>1.186350e+05</td>\n",
       "      <td>4.878793e+04</td>\n",
       "      <td>3.863322e+06</td>\n",
       "      <td>3.863322e+06</td>\n",
       "      <td>3.645960e+06</td>\n",
       "      <td>2.034750e+04</td>\n",
       "      <td>1.367400e+04</td>\n",
       "      <td>68004.500000</td>\n",
       "      <td>7.045800e+05</td>\n",
       "      <td>3.364634e+06</td>\n",
       "      <td>8.723550e+04</td>\n",
       "      <td>2.947266e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.493523e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.936000e+03</td>\n",
       "      <td>2.320660e+05</td>\n",
       "      <td>1.207694e+06</td>\n",
       "      <td>1.519808e+06</td>\n",
       "      <td>4.490600e+05</td>\n",
       "      <td>3.107400e+04</td>\n",
       "      <td>5.286000e+03</td>\n",
       "      <td>2.940000e+02</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.863322e+06</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>692.000000</td>\n",
       "      <td>9.492400e+04</td>\n",
       "      <td>2.959090e+05</td>\n",
       "      <td>2.085000e+05</td>\n",
       "      <td>2.049318e+06</td>\n",
       "      <td>6.862600e+05</td>\n",
       "      <td>1.816400e+04</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.734000e+04</td>\n",
       "      <td>3.767200e+04</td>\n",
       "      <td>8319.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.920200e+04</td>\n",
       "      <td>17630.000000</td>\n",
       "      <td>3.472540e+06</td>\n",
       "      <td>5.346555e+05</td>\n",
       "      <td>8.764000e+03</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000270e+06</td>\n",
       "      <td>5.730600e+05</td>\n",
       "      <td>6.673900e+05</td>\n",
       "      <td>4.383960e+05</td>\n",
       "      <td>2.182320e+05</td>\n",
       "      <td>4.666140e+05</td>\n",
       "      <td>4.032220e+05</td>\n",
       "      <td>2.750940e+05</td>\n",
       "      <td>1.678140e+05</td>\n",
       "      <td>1.397240e+05</td>\n",
       "      <td>2.028000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.746564e+06</td>\n",
       "      <td>2.130707e+09</td>\n",
       "      <td>21050.000000</td>\n",
       "      <td>20070.000000</td>\n",
       "      <td>3.376892e+06</td>\n",
       "      <td>4.109372e+06</td>\n",
       "      <td>1.055286e+07</td>\n",
       "      <td>6.340207e+07</td>\n",
       "      <td>2.288306e+08</td>\n",
       "      <td>1.791880e+08</td>\n",
       "      <td>9.402067e+07</td>\n",
       "      <td>6.334675e+07</td>\n",
       "      <td>1.770252e+07</td>\n",
       "      <td>2.519851e+07</td>\n",
       "      <td>7.424732e+07</td>\n",
       "      <td>1.651285e+07</td>\n",
       "      <td>5.629340e+06</td>\n",
       "      <td>1.044492e+07</td>\n",
       "      <td>3.476258e+07</td>\n",
       "      <td>5.590351e+07</td>\n",
       "      <td>1.408618e+08</td>\n",
       "      <td>1.222018e+08</td>\n",
       "      <td>7.793494e+07</td>\n",
       "      <td>2.556265e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>1.655240e+06</td>\n",
       "      <td>1.040050e+07</td>\n",
       "      <td>2.626676e+06</td>\n",
       "      <td>794458.000000</td>\n",
       "      <td>116652.000000</td>\n",
       "      <td>5.055389e+07</td>\n",
       "      <td>8.052538e+07</td>\n",
       "      <td>2.847484e+07</td>\n",
       "      <td>1.394517e+07</td>\n",
       "      <td>4.002870e+07</td>\n",
       "      <td>1.249489e+08</td>\n",
       "      <td>1.276803e+08</td>\n",
       "      <td>4.896782e+08</td>\n",
       "      <td>1.045670e+08</td>\n",
       "      <td>1.882466e+07</td>\n",
       "      <td>1.012462e+07</td>\n",
       "      <td>4.530258e+06</td>\n",
       "      <td>1.421766e+07</td>\n",
       "      <td>4.558424e+07</td>\n",
       "      <td>1.230471e+08</td>\n",
       "      <td>4.678323e+08</td>\n",
       "      <td>6.458914e+07</td>\n",
       "      <td>3.915822e+07</td>\n",
       "      <td>1.947884e+06</td>\n",
       "      <td>666148.000000</td>\n",
       "      <td>2.328717e+08</td>\n",
       "      <td>1.162833e+08</td>\n",
       "      <td>5.580739e+07</td>\n",
       "      <td>3.693142e+07</td>\n",
       "      <td>2.515856e+07</td>\n",
       "      <td>1.920866e+07</td>\n",
       "      <td>1.899766e+07</td>\n",
       "      <td>1.431409e+07</td>\n",
       "      <td>3.126598e+07</td>\n",
       "      <td>4.370641e+07</td>\n",
       "      <td>1.928715e+08</td>\n",
       "      <td>396952.000000</td>\n",
       "      <td>306452.000000</td>\n",
       "      <td>810568.000000</td>\n",
       "      <td>51050.000000</td>\n",
       "      <td>7.424732e+07</td>\n",
       "      <td>3.200582e+06</td>\n",
       "      <td>4.493750e+07</td>\n",
       "      <td>4.573632e+07</td>\n",
       "      <td>1.037240e+06</td>\n",
       "      <td>2.746565e+06</td>\n",
       "      <td>1.928715e+08</td>\n",
       "      <td>1.928715e+08</td>\n",
       "      <td>1.863539e+08</td>\n",
       "      <td>1.002003e+06</td>\n",
       "      <td>4.054259e+07</td>\n",
       "      <td>120956.000000</td>\n",
       "      <td>1.209520e+06</td>\n",
       "      <td>1.486152e+08</td>\n",
       "      <td>4.908098e+06</td>\n",
       "      <td>1.409861e+08</td>\n",
       "      <td>6.094967e+07</td>\n",
       "      <td>5.542867e+07</td>\n",
       "      <td>6.278490e+06</td>\n",
       "      <td>1.451299e+07</td>\n",
       "      <td>5.850861e+07</td>\n",
       "      <td>9.497932e+07</td>\n",
       "      <td>1.698693e+08</td>\n",
       "      <td>1.178158e+08</td>\n",
       "      <td>7.208041e+07</td>\n",
       "      <td>3.314373e+07</td>\n",
       "      <td>7.541716e+06</td>\n",
       "      <td>3.639837e+07</td>\n",
       "      <td>496360.000000</td>\n",
       "      <td>1.928715e+08</td>\n",
       "      <td>839240.000000</td>\n",
       "      <td>438806.000000</td>\n",
       "      <td>4.608594e+07</td>\n",
       "      <td>4.242185e+07</td>\n",
       "      <td>7.486063e+07</td>\n",
       "      <td>3.791421e+08</td>\n",
       "      <td>7.374197e+07</td>\n",
       "      <td>1.288422e+07</td>\n",
       "      <td>1.584558e+06</td>\n",
       "      <td>4.490299e+07</td>\n",
       "      <td>445142.000000</td>\n",
       "      <td>176176.000000</td>\n",
       "      <td>2.161391e+07</td>\n",
       "      <td>2.706429e+07</td>\n",
       "      <td>1.247009e+08</td>\n",
       "      <td>2.298742e+07</td>\n",
       "      <td>726750.000000</td>\n",
       "      <td>5.483574e+06</td>\n",
       "      <td>1.038581e+08</td>\n",
       "      <td>2.369792e+07</td>\n",
       "      <td>2.924584e+06</td>\n",
       "      <td>1.874542e+06</td>\n",
       "      <td>348118.000000</td>\n",
       "      <td>6.351873e+09</td>\n",
       "      <td>5.013766e+07</td>\n",
       "      <td>4.970962e+06</td>\n",
       "      <td>656432.000000</td>\n",
       "      <td>4.602076e+08</td>\n",
       "      <td>1.270345e+08</td>\n",
       "      <td>1.142884e+08</td>\n",
       "      <td>3.793022e+06</td>\n",
       "      <td>1414.000000</td>\n",
       "      <td>8506.000000</td>\n",
       "      <td>1.322457e+09</td>\n",
       "      <td>7.498445e+07</td>\n",
       "      <td>9.822438e+07</td>\n",
       "      <td>7.793393e+07</td>\n",
       "      <td>3.775839e+07</td>\n",
       "      <td>9.715238e+07</td>\n",
       "      <td>5.743524e+07</td>\n",
       "      <td>3.160781e+07</td>\n",
       "      <td>1.195801e+08</td>\n",
       "      <td>1.926740e+07</td>\n",
       "      <td>3.810078e+06</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>1146.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             aa_000        ac_000        ae_000        af_000        ag_000  \\\n",
       "count  6.000000e+04  5.666500e+04  57500.000000  57500.000000  5.932900e+04   \n",
       "mean   5.933650e+04  3.560143e+08      6.819130     11.006817  2.216364e+02   \n",
       "std    1.454301e+05  7.948749e+08    161.543373    209.792592  2.047846e+04   \n",
       "min    0.000000e+00  0.000000e+00      0.000000      0.000000  0.000000e+00   \n",
       "25%    8.340000e+02  1.600000e+01      0.000000      0.000000  0.000000e+00   \n",
       "50%    3.077600e+04  1.520000e+02      0.000000      0.000000  0.000000e+00   \n",
       "75%    4.866800e+04  9.640000e+02      0.000000      0.000000  0.000000e+00   \n",
       "max    2.746564e+06  2.130707e+09  21050.000000  20070.000000  3.376892e+06   \n",
       "\n",
       "             ag_001        ag_002        ag_003        ag_004        ag_005  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
       "mean   9.757223e+02  8.606015e+03  8.859128e+04  4.370966e+05  1.108374e+06   \n",
       "std    3.420053e+04  1.503220e+05  7.617312e+05  2.374282e+06  3.262607e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  3.080000e+02  1.383400e+04   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  3.672000e+03  1.760200e+05   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  4.952200e+04  9.139640e+05   \n",
       "max    4.109372e+06  1.055286e+07  6.340207e+07  2.288306e+08  1.791880e+08   \n",
       "\n",
       "             ag_006        ag_007        ag_008        ag_009        ah_000  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.935500e+04   \n",
       "mean   1.657818e+06  4.993098e+05  3.556989e+04  5.114753e+03  1.809931e+06   \n",
       "std    3.909384e+06  1.422765e+06  2.201524e+05  1.696582e+05  4.185740e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    1.060800e+04  0.000000e+00  0.000000e+00  0.000000e+00  2.973300e+04   \n",
       "50%    9.303360e+05  1.192040e+05  1.786000e+03  0.000000e+00  1.002420e+06   \n",
       "75%    1.886608e+06  5.888200e+05  2.669000e+04  3.640000e+02  1.601366e+06   \n",
       "max    9.402067e+07  6.334675e+07  1.770252e+07  2.519851e+07  7.424732e+07   \n",
       "\n",
       "             ai_000        aj_000        ak_000        al_000          am_0  \\\n",
       "count  5.937100e+04  5.937100e+04  5.560000e+04  5.935800e+04  5.937100e+04   \n",
       "mean   9.016965e+03  1.143675e+03  9.794900e+02  5.913048e+04  9.328133e+04   \n",
       "std    1.632778e+05  5.035971e+04  7.583162e+04  5.394658e+05  8.494694e+05   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  1.204000e+03  2.364000e+03   \n",
       "max    1.651285e+07  5.629340e+06  1.044492e+07  3.476258e+07  5.590351e+07   \n",
       "\n",
       "             an_000        ao_000        ap_000        aq_000        ar_000  \\\n",
       "count  5.935800e+04  5.941100e+04  5.935800e+04  5.941100e+04  57277.000000   \n",
       "mean   3.461037e+06  3.002440e+06  1.004160e+06  4.424045e+05      0.496918   \n",
       "std    7.790350e+06  6.819518e+06  3.088457e+06  1.262469e+06      5.511653   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      0.000000   \n",
       "25%    7.323850e+04  6.558500e+04  2.518900e+04  4.161000e+03      0.000000   \n",
       "50%    1.918629e+06  1.643556e+06  3.572810e+05  1.787920e+05      0.000000   \n",
       "75%    3.128416e+06  2.675796e+06  7.246605e+05  3.769000e+05      0.000000   \n",
       "max    1.408618e+08  1.222018e+08  7.793494e+07  2.556265e+07    350.000000   \n",
       "\n",
       "             as_000        at_000        au_000         av_000         ax_000  \\\n",
       "count  5.937100e+04  5.937100e+04  5.937100e+04   57500.000000   57499.000000   \n",
       "mean   1.267365e+02  5.072046e+03  2.305804e+02    1117.825913     374.327380   \n",
       "std    1.101004e+04  1.196159e+05  1.579952e+04    6598.611557    1482.711621   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00       0.000000       0.000000   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00      12.000000      10.000000   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00     116.000000      66.000000   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00     646.000000     263.000000   \n",
       "max    1.655240e+06  1.040050e+07  2.626676e+06  794458.000000  116652.000000   \n",
       "\n",
       "             ay_000        ay_001        ay_002        ay_003        ay_004  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
       "mean   1.221165e+04  1.019012e+04  1.097500e+04  7.225784e+03  1.056600e+04   \n",
       "std    4.544963e+05  5.352707e+05  4.283370e+05  2.064679e+05  3.546258e+05   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    5.055389e+07  8.052538e+07  2.847484e+07  1.394517e+07  4.002870e+07   \n",
       "\n",
       "             ay_005        ay_006        ay_007        ay_008        ay_009  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
       "mean   1.119791e+05  1.078551e+06  1.546032e+06  1.051123e+06  1.162622e+03   \n",
       "std    1.394585e+06  3.278941e+06  5.106177e+06  3.991050e+06  9.796080e+04   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  6.118000e+03  7.524000e+03  0.000000e+00   \n",
       "50%    0.000000e+00  1.682020e+05  3.486220e+05  9.481200e+04  0.000000e+00   \n",
       "75%    4.013200e+04  1.270244e+06  1.337364e+06  6.118160e+05  0.000000e+00   \n",
       "max    1.249489e+08  1.276803e+08  4.896782e+08  1.045670e+08  1.882466e+07   \n",
       "\n",
       "             az_000        az_001        az_002        az_003        az_004  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
       "mean   7.849608e+03  4.420992e+03  8.066082e+03  8.724082e+04  1.476897e+06   \n",
       "std    7.363676e+04  3.399539e+04  1.065997e+05  6.532562e+05  4.184087e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    1.028000e+03  6.000000e+01  9.000000e+01  2.940000e+02  1.542000e+03   \n",
       "50%    2.098000e+03  6.360000e+02  1.016000e+03  3.570000e+03  8.161400e+04   \n",
       "75%    4.150000e+03  2.016000e+03  3.136000e+03  4.301400e+04  1.774410e+06   \n",
       "max    1.012462e+07  4.530258e+06  1.421766e+07  4.558424e+07  1.230471e+08   \n",
       "\n",
       "             az_005        az_006        az_007        az_008         az_009  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   59329.000000   \n",
       "mean   2.135584e+06  1.018943e+05  1.737782e+04  6.617861e+02      42.073455   \n",
       "std    6.460221e+06  8.997699e+05  2.804446e+05  1.492475e+04    3256.569903   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%    3.863800e+04  1.000000e+01  0.000000e+00  0.000000e+00       0.000000   \n",
       "50%    5.270340e+05  2.920000e+02  0.000000e+00  0.000000e+00       0.000000   \n",
       "75%    1.796172e+06  4.218000e+03  0.000000e+00  0.000000e+00       0.000000   \n",
       "max    4.678323e+08  6.458914e+07  3.915822e+07  1.947884e+06  666148.000000   \n",
       "\n",
       "             ba_000        ba_001        ba_002        ba_003        ba_004  \\\n",
       "count  5.931200e+04  5.931200e+04  5.931200e+04  5.931200e+04  5.931200e+04   \n",
       "mean   1.399652e+06  8.941175e+05  4.130969e+05  2.740070e+05  2.048756e+05   \n",
       "std    3.777091e+06  2.346002e+06  1.196083e+06  7.485388e+05  5.390584e+05   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    3.347850e+04  1.483350e+04  5.207500e+03  1.842000e+03  6.120000e+02   \n",
       "50%    6.794860e+05  4.436410e+05  1.860460e+05  1.341490e+05  1.018740e+05   \n",
       "75%    1.276368e+06  8.110005e+05  3.404085e+05  2.444490e+05  1.971620e+05   \n",
       "max    2.328717e+08  1.162833e+08  5.580739e+07  3.693142e+07  2.515856e+07   \n",
       "\n",
       "             ba_005        ba_006        ba_007        ba_008        ba_009  \\\n",
       "count  5.931200e+04  5.931200e+04  5.931200e+04  5.931200e+04  5.931200e+04   \n",
       "mean   1.889412e+05  2.106288e+05  1.857874e+05  3.588284e+04  3.576672e+04   \n",
       "std    5.092672e+05  6.370815e+05  5.251506e+05  2.436657e+05  3.353276e+05   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    3.740000e+02  3.520000e+02  7.200000e+01  0.000000e+00  0.000000e+00   \n",
       "50%    8.399200e+04  7.011600e+04  4.470000e+03  2.200000e+01  0.000000e+00   \n",
       "75%    1.846740e+05  2.049590e+05  2.074710e+05  1.820000e+03  6.000000e+01   \n",
       "max    1.920866e+07  1.899766e+07  1.431409e+07  3.126598e+07  4.370641e+07   \n",
       "\n",
       "             bb_000         bc_000         bd_000         be_000  \\\n",
       "count  5.935500e+04   57275.000000   57273.000000   57497.000000   \n",
       "mean   4.526177e+06     569.526565     921.775461    1372.646086   \n",
       "std    1.088674e+07    4045.937032    4833.065431    9249.936515   \n",
       "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
       "25%    1.056010e+05       0.000000       8.000000      18.000000   \n",
       "50%    2.360728e+06      16.000000      66.000000     180.000000   \n",
       "75%    3.868370e+06     136.000000     438.000000     614.000000   \n",
       "max    1.928715e+08  396952.000000  306452.000000  810568.000000   \n",
       "\n",
       "             bf_000        bg_000        bh_000        bi_000        bj_000  \\\n",
       "count  57500.000000  5.935800e+04  5.935800e+04  5.941100e+04  5.941100e+04   \n",
       "mean      74.878261  1.809431e+06  5.794308e+04  4.922076e+05  5.100892e+05   \n",
       "std      546.789611  4.180200e+06  1.522093e+05  1.485185e+06  1.820105e+06   \n",
       "min        0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%        0.000000  2.974300e+04  8.520000e+02  1.594700e+04  8.522000e+03   \n",
       "50%        2.000000  1.002718e+06  2.635200e+04  1.798420e+05  1.544040e+05   \n",
       "75%       18.000000  1.602766e+06  4.908550e+04  3.796100e+05  3.336080e+05   \n",
       "max    51050.000000  7.424732e+07  3.200582e+06  4.493750e+07  4.573632e+07   \n",
       "\n",
       "             bs_000        bt_000        bu_000        bv_000        bx_000  \\\n",
       "count  5.927400e+04  5.983300e+04  5.930900e+04  5.930900e+04  5.674300e+04   \n",
       "mean   8.036055e+04  5.941650e+04  4.515325e+06  4.515325e+06  4.112218e+06   \n",
       "std    8.451276e+04  1.454464e+05  1.085990e+07  1.085990e+07  1.035154e+07   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.720000e+02   \n",
       "25%    1.730000e+04  8.628300e+02  1.054440e+05  1.054440e+05  8.964900e+04   \n",
       "50%    5.054000e+04  3.083986e+04  2.359656e+06  2.359656e+06  2.258824e+06   \n",
       "75%    1.186350e+05  4.878793e+04  3.863322e+06  3.863322e+06  3.645960e+06   \n",
       "max    1.037240e+06  2.746565e+06  1.928715e+08  1.928715e+08  1.863539e+08   \n",
       "\n",
       "             by_000        bz_000         ca_000        cb_000        cc_000  \\\n",
       "count  5.952700e+04  5.727700e+04   55644.000000  5.927400e+04  5.674500e+04   \n",
       "mean   2.202893e+04  1.019608e+05   39168.817123  4.056381e+05  3.803444e+06   \n",
       "std    5.399282e+04  6.289129e+05   36748.302141  3.693868e+05  9.625672e+06   \n",
       "min    0.000000e+00  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
       "25%    2.160000e+02  6.000000e+00    6886.000000  7.712500e+04  6.241600e+04   \n",
       "50%    1.262800e+04  1.036000e+03   25436.000000  2.789900e+05  2.108912e+06   \n",
       "75%    2.034750e+04  1.367400e+04   68004.500000  7.045800e+05  3.364634e+06   \n",
       "max    1.002003e+06  4.054259e+07  120956.000000  1.209520e+06  1.486152e+08   \n",
       "\n",
       "             ce_000        ci_000        cj_000        ck_000        cn_000  \\\n",
       "count  5.749800e+04  5.966200e+04  5.966200e+04  5.966200e+04  5.931300e+04   \n",
       "mean   6.434356e+04  3.481204e+06  1.028419e+05  7.143427e+05  2.336674e+03   \n",
       "std    1.428469e+05  8.355997e+06  1.135174e+06  2.180714e+06  6.118383e+04   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    2.660000e+02  4.824984e+04  0.000000e+00  1.458672e+04  0.000000e+00   \n",
       "50%    3.409000e+03  1.858641e+06  0.000000e+00  2.502672e+05  0.000000e+00   \n",
       "75%    8.723550e+04  2.947266e+06  0.000000e+00  5.493523e+05  0.000000e+00   \n",
       "max    4.908098e+06  1.409861e+08  6.094967e+07  5.542867e+07  6.278490e+06   \n",
       "\n",
       "             cn_001        cn_002        cn_003        cn_004        cn_005  \\\n",
       "count  5.931300e+04  5.931300e+04  5.931300e+04  5.931300e+04  5.931300e+04   \n",
       "mean   2.195149e+04  1.610509e+05  5.314780e+05  1.282835e+06  1.341059e+06   \n",
       "std    2.457994e+05  1.073893e+06  2.216301e+06  3.357254e+06  3.144659e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  4.622000e+03  1.911400e+04  5.028000e+03   \n",
       "50%    0.000000e+00  0.000000e+00  3.499400e+04  5.184620e+05  7.035240e+05   \n",
       "75%    0.000000e+00  7.936000e+03  2.320660e+05  1.207694e+06  1.519808e+06   \n",
       "max    1.451299e+07  5.850861e+07  9.497932e+07  1.698693e+08  1.178158e+08   \n",
       "\n",
       "             cn_006        cn_007        cn_008        cn_009         cp_000  \\\n",
       "count  5.931300e+04  5.931300e+04  5.931300e+04  5.931300e+04   57276.000000   \n",
       "mean   4.105641e+05  6.442513e+04  1.922679e+04  7.820467e+03     570.404288   \n",
       "std    1.294208e+06  4.064541e+05  1.862697e+05  2.664928e+05    7773.264479   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%    6.260000e+02  6.200000e+01  0.000000e+00  0.000000e+00       4.000000   \n",
       "50%    9.626600e+04  9.976000e+03  1.852000e+03  2.400000e+01      14.000000   \n",
       "75%    4.490600e+05  3.107400e+04  5.286000e+03  2.940000e+02      82.000000   \n",
       "max    7.208041e+07  3.314373e+07  7.541716e+06  3.639837e+07  496360.000000   \n",
       "\n",
       "             cq_000         cs_000         cs_001        cs_002        cs_003  \\\n",
       "count  5.930900e+04   59331.000000   59331.000000  5.933100e+04  5.933100e+04   \n",
       "mean   4.515325e+06    5479.857073     788.425545  2.388106e+05  3.553731e+05   \n",
       "std    1.085990e+07   10313.977722    2886.331336  1.215538e+06  1.110964e+06   \n",
       "min    0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
       "25%    1.054440e+05    1232.000000      32.000000  2.220000e+02  3.079000e+03   \n",
       "50%    2.359656e+06    3192.000000     360.000000  2.057000e+04  1.217800e+05   \n",
       "75%    3.863322e+06    5686.000000     692.000000  9.492400e+04  2.959090e+05   \n",
       "max    1.928715e+08  839240.000000  438806.000000  4.608594e+07  4.242185e+07   \n",
       "\n",
       "             cs_004        cs_005        cs_006        cs_007        cs_008  \\\n",
       "count  5.933100e+04  5.933100e+04  5.933100e+04  5.933100e+04  5.933100e+04   \n",
       "mean   4.442283e+05  2.235387e+06  5.457742e+05  1.477142e+04  2.117473e+02   \n",
       "std    2.073937e+06  5.613545e+06  1.168199e+06  8.005122e+04  1.015337e+04   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    2.726000e+03  1.918600e+04  1.336000e+04  1.204000e+03  2.000000e+00   \n",
       "50%    9.108000e+04  1.220860e+06  2.407440e+05  6.104000e+03  4.600000e+01   \n",
       "75%    2.085000e+05  2.049318e+06  6.862600e+05  1.816400e+04  1.480000e+02   \n",
       "max    7.486063e+07  3.791421e+08  7.374197e+07  1.288422e+07  1.584558e+06   \n",
       "\n",
       "             cs_009         dd_000         de_000        df_000        dg_000  \\\n",
       "count  5.933100e+04   57497.000000   57276.000000  5.599200e+04  5.599200e+04   \n",
       "mean   7.791978e+02    3123.961911     375.147112  2.718638e+03  5.609957e+03   \n",
       "std    1.843585e+05    9516.675102    1689.062059  1.373331e+05  2.085649e+05   \n",
       "min    0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00     132.000000      66.000000  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00    1354.000000     144.000000  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00    2678.000000     296.000000  0.000000e+00  0.000000e+00   \n",
       "max    4.490299e+07  445142.000000  176176.000000  2.161391e+07  2.706429e+07   \n",
       "\n",
       "             dh_000        di_000         dj_000        dk_000        dl_000  \\\n",
       "count  5.599200e+04  5.599400e+04   55993.000000  5.599300e+04  5.599200e+04   \n",
       "mean   4.707073e+03  3.724824e+04      39.938564  1.861313e+03  2.854177e+04   \n",
       "std    5.602799e+05  4.254382e+05    4533.142182  6.659786e+04  1.095662e+06   \n",
       "min    0.000000e+00  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
       "max    1.247009e+08  2.298742e+07  726750.000000  5.483574e+06  1.038581e+08   \n",
       "\n",
       "             dm_000        dn_000        do_000         dp_000        dq_000  \\\n",
       "count  5.599100e+04  5.930900e+04  5.727600e+04   57274.000000  5.727400e+04   \n",
       "mean   7.923228e+03  3.374545e+04  2.850785e+04    6958.652722  4.529375e+06   \n",
       "std    2.775356e+05  9.733719e+04  6.125476e+04   13955.451781  9.748478e+07   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00       0.000000  0.000000e+00   \n",
       "25%    0.000000e+00  6.600000e+02  2.000000e+01       6.000000  0.000000e+00   \n",
       "50%    0.000000e+00  1.433000e+04  1.037700e+04    2532.000000  0.000000e+00   \n",
       "75%    0.000000e+00  2.734000e+04  3.767200e+04    8319.500000  0.000000e+00   \n",
       "max    2.369792e+07  2.924584e+06  1.874542e+06  348118.000000  6.351873e+09   \n",
       "\n",
       "             dr_000        ds_000         dt_000        du_000        dv_000  \\\n",
       "count  5.727400e+04  5.727300e+04   57273.000000  5.727400e+04  5.727400e+04   \n",
       "mean   2.037598e+05  8.965500e+04   15403.354670  4.058712e+06  5.938350e+05   \n",
       "std    1.366065e+06  2.082016e+05   33801.022975  1.156777e+07  2.082998e+06   \n",
       "min    0.000000e+00  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  6.840000e+02     150.000000  5.380000e+03  7.420000e+02   \n",
       "50%    0.000000e+00  4.794000e+04    8316.000000  1.854000e+05  3.059200e+04   \n",
       "75%    0.000000e+00  9.920200e+04   17630.000000  3.472540e+06  5.346555e+05   \n",
       "max    5.013766e+07  4.970962e+06  656432.000000  4.602076e+08  1.270345e+08   \n",
       "\n",
       "             dx_000        dy_000        dz_000        ea_000        eb_000  \\\n",
       "count  5.727700e+04  5.727600e+04  57277.000000  57277.000000  5.599300e+04   \n",
       "mean   7.912085e+05  7.780350e+03      0.215759      1.567750  9.717093e+06   \n",
       "std    4.151033e+06  5.924449e+04     10.821035     53.528722  4.274675e+07   \n",
       "min    0.000000e+00  0.000000e+00      0.000000      0.000000  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00      0.000000      0.000000  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00      0.000000      0.000000  6.221100e+05   \n",
       "75%    8.764000e+03  3.600000e+01      0.000000      0.000000  4.000270e+06   \n",
       "max    1.142884e+08  3.793022e+06   1414.000000   8506.000000  1.322457e+09   \n",
       "\n",
       "             ee_000        ee_001        ee_002        ee_003        ee_004  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
       "mean   7.334042e+05  7.838746e+05  4.454897e+05  2.111264e+05  4.457343e+05   \n",
       "std    2.416166e+06  2.570111e+06  1.155540e+06  5.433188e+05  1.168314e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    1.569800e+04  8.536000e+03  2.936000e+03  1.166000e+03  2.700000e+03   \n",
       "50%    2.607040e+05  3.469400e+05  2.337960e+05  1.120860e+05  2.215180e+05   \n",
       "75%    5.730600e+05  6.673900e+05  4.383960e+05  2.182320e+05  4.666140e+05   \n",
       "max    7.498445e+07  9.822438e+07  7.793393e+07  3.775839e+07  9.715238e+07   \n",
       "\n",
       "             ee_005        ee_006        ee_007        ee_008        ee_009  \\\n",
       "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
       "mean   3.939462e+05  3.330582e+05  3.462714e+05  1.387300e+05  8.388915e+03   \n",
       "std    1.121044e+06  1.069160e+06  1.728056e+06  4.495100e+05  4.747043e+04   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    3.584000e+03  5.120000e+02  1.100000e+02  0.000000e+00  0.000000e+00   \n",
       "50%    1.899880e+05  9.243200e+04  4.109800e+04  3.812000e+03  0.000000e+00   \n",
       "75%    4.032220e+05  2.750940e+05  1.678140e+05  1.397240e+05  2.028000e+03   \n",
       "max    5.743524e+07  3.160781e+07  1.195801e+08  1.926740e+07  3.810078e+06   \n",
       "\n",
       "             ef_000        eg_000  \n",
       "count  57276.000000  57277.000000  \n",
       "mean       0.090579      0.212756  \n",
       "std        4.368855      8.830641  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%        0.000000      0.000000  \n",
       "75%        0.000000      0.000000  \n",
       "max      482.000000   1146.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500fc14d-e6bb-474c-8ea4-29ab0907e9b1",
   "metadata": {},
   "source": [
    "## Preprocessamento para treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909e9ccd-0dee-48a2-a954-8d435e9a0a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:28.504425Z",
     "iopub.status.busy": "2024-07-20T13:55:28.504093Z",
     "iopub.status.idle": "2024-07-20T13:55:28.516664Z",
     "shell.execute_reply": "2024-07-20T13:55:28.516349Z",
     "shell.execute_reply.started": "2024-07-20T13:55:28.504408Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"class\", axis=1) # Variveis independentes\n",
    "y = df[\"class\"] # Varivel dependente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f73207c-5a18-4445-b2e8-bde033612482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:29.472447Z",
     "iopub.status.busy": "2024-07-20T13:55:29.472230Z",
     "iopub.status.idle": "2024-07-20T13:55:29.474951Z",
     "shell.execute_reply": "2024-07-20T13:55:29.474539Z",
     "shell.execute_reply.started": "2024-07-20T13:55:29.472431Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_transform(y):\n",
    "    return y.replace({'neg': 0, 'pos': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2828246-8d1a-421e-b873-8213c71e3874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:33.944207Z",
     "iopub.status.busy": "2024-07-20T13:55:33.944030Z",
     "iopub.status.idle": "2024-07-20T13:55:33.958178Z",
     "shell.execute_reply": "2024-07-20T13:55:33.957752Z",
     "shell.execute_reply.started": "2024-07-20T13:55:33.944194Z"
    }
   },
   "outputs": [],
   "source": [
    "y_trans = label_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3a8cd92-ed29-4671-9560-c67b1bd5a24f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:55:34.344078Z",
     "iopub.status.busy": "2024-07-20T13:55:34.343910Z",
     "iopub.status.idle": "2024-07-20T13:55:34.346184Z",
     "shell.execute_reply": "2024-07-20T13:55:34.345878Z",
     "shell.execute_reply.started": "2024-07-20T13:55:34.344066Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), numerical_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d1f6c0-8a2f-420f-85b3-918e411ee8f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T14:02:43.875972Z",
     "iopub.status.busy": "2024-07-20T14:02:43.875587Z",
     "iopub.status.idle": "2024-07-20T14:02:43.878901Z",
     "shell.execute_reply": "2024-07-20T14:02:43.878550Z",
     "shell.execute_reply.started": "2024-07-20T14:02:43.875957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testando varios modelos para escolher aquele que teve o melhor recall\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=321),\n",
    "    # 'SVM': SVC(probability=True, random_state=321),\n",
    "    # 'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=321),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=321),\n",
    "    'XGBoost': XGBClassifier(random_state=321),\n",
    "    'LightGBM': LGBMClassifier(random_state=321),\n",
    "    'CatBoost': CatBoostClassifier(random_state=321, verbose=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f01a45b-a2f8-4356-b862-b8db088a4bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T14:02:52.520863Z",
     "iopub.status.busy": "2024-07-20T14:02:52.520632Z",
     "iopub.status.idle": "2024-07-20T14:02:52.523430Z",
     "shell.execute_reply": "2024-07-20T14:02:52.523136Z",
     "shell.execute_reply.started": "2024-07-20T14:02:52.520852Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_models_pipeline(preprocessor, bal, scaler, threshold, sel_algorithm, dim_reduc, model, X, y):\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('balancer', bal),\n",
    "        ('scaler', scaler),\n",
    "        ('threshold', threshold),\n",
    "        ('feature_selection', sel_algorithm),\n",
    "        ('dim_reduction_algorithm', dim_reduc),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    cross_validation = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cross_validation, scoring='f1')\n",
    "\n",
    "    return [scores.mean(), pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28429227-6339-4f4c-805c-872c94e99900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T14:03:02.480111Z",
     "iopub.status.busy": "2024-07-20T14:03:02.479726Z",
     "iopub.status.idle": "2024-07-20T14:22:15.053562Z",
     "shell.execute_reply": "2024-07-20T14:22:15.053274Z",
     "shell.execute_reply.started": "2024-07-20T14:03:02.480094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Recall Mdio: 0.4810634909011619\n",
      "Gradient Boosting Recall Mdio: 0.4266642253677733\n",
      "Random Forest Recall Mdio: 0.6114101768341218\n",
      "AdaBoost Recall Mdio: 0.3680412689483469\n",
      "XGBoost Recall Mdio: 0.629249564953945\n",
      "[LightGBM] [Info] Number of positive: 47197, number of negative: 47197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 94394, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47202, number of negative: 47202\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 94404, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47197, number of negative: 47197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 94394, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47196, number of negative: 47196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 94392, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47208, number of negative: 47208\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 94416, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM Recall Mdio: 0.5551779787151109\n",
      "CatBoost Recall Mdio: 0.6292322626865481\n"
     ]
    }
   ],
   "source": [
    "# Compara f1_score usando validao cruzada para alguns modelos de classificao\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    score_and_pipeline = run_models_pipeline(preprocessor, over_sampling.SMOTE(random_state=42),\n",
    "                               StandardScaler(with_mean=False), VarianceThreshold(0.01),\n",
    "                               SmartCorrelatedSelection(threshold=0.8, selection_method='variance'),\n",
    "                               PCA(n_components=38), model, X, y_trans)\n",
    "\n",
    "    all_scores[name] = score_and_pipeline\n",
    "    print(f\"{name} Recall Mdio: {score_and_pipeline[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3443194f-bb0c-4e1e-a235-e4bb8cca3f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T15:02:48.377955Z",
     "iopub.status.busy": "2024-07-20T15:02:48.377771Z",
     "iopub.status.idle": "2024-07-20T15:02:48.380200Z",
     "shell.execute_reply": "2024-07-20T15:02:48.379912Z",
     "shell.execute_reply.started": "2024-07-20T15:02:48.377944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor modelo: XGBoost\n"
     ]
    }
   ],
   "source": [
    "best_score = max(list(all_scores.items()), key=lambda x: x[1])\n",
    "best_model = best_score[0]\n",
    "print(f\"Melhor modelo:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2b6f826-c214-4ca5-8319-db2d3a332169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T15:02:50.110915Z",
     "iopub.status.busy": "2024-07-20T15:02:50.110747Z",
     "iopub.status.idle": "2024-07-20T15:02:50.125301Z",
     "shell.execute_reply": "2024-07-20T15:02:50.124952Z",
     "shell.execute_reply.started": "2024-07-20T15:02:50.110903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;),\n",
       "                                                  [&#x27;aa_000&#x27;, &#x27;ac_000&#x27;, &#x27;ae_000&#x27;,\n",
       "                                                   &#x27;af_000&#x27;, &#x27;ag_000&#x27;, &#x27;ag_001&#x27;,\n",
       "                                                   &#x27;ag_002&#x27;, &#x27;ag_003&#x27;, &#x27;ag_004&#x27;,\n",
       "                                                   &#x27;ag_005&#x27;, &#x27;ag_006&#x27;, &#x27;ag_007&#x27;,\n",
       "                                                   &#x27;ag_008&#x27;, &#x27;ag_009&#x27;, &#x27;ah_000&#x27;,\n",
       "                                                   &#x27;ai_000&#x27;, &#x27;aj_000&#x27;, &#x27;ak_000&#x27;,\n",
       "                                                   &#x27;al_000&#x27;, &#x27;am_0&#x27;, &#x27;an_000&#x27;,\n",
       "                                                   &#x27;ao_000&#x27;, &#x27;ap_000&#x27;, &#x27;aq_000&#x27;,\n",
       "                                                   &#x27;ar_000&#x27;, &#x27;as_000&#x27;, &#x27;at_000&#x27;,\n",
       "                                                   &#x27;au_...\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=None, n_jobs=None,\n",
       "                               num_parallel_tree=None, random_state=321, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;Pipeline<span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;),\n",
       "                                                  [&#x27;aa_000&#x27;, &#x27;ac_000&#x27;, &#x27;ae_000&#x27;,\n",
       "                                                   &#x27;af_000&#x27;, &#x27;ag_000&#x27;, &#x27;ag_001&#x27;,\n",
       "                                                   &#x27;ag_002&#x27;, &#x27;ag_003&#x27;, &#x27;ag_004&#x27;,\n",
       "                                                   &#x27;ag_005&#x27;, &#x27;ag_006&#x27;, &#x27;ag_007&#x27;,\n",
       "                                                   &#x27;ag_008&#x27;, &#x27;ag_009&#x27;, &#x27;ah_000&#x27;,\n",
       "                                                   &#x27;ai_000&#x27;, &#x27;aj_000&#x27;, &#x27;ak_000&#x27;,\n",
       "                                                   &#x27;al_000&#x27;, &#x27;am_0&#x27;, &#x27;an_000&#x27;,\n",
       "                                                   &#x27;ao_000&#x27;, &#x27;ap_000&#x27;, &#x27;aq_000&#x27;,\n",
       "                                                   &#x27;ar_000&#x27;, &#x27;as_000&#x27;, &#x27;at_000&#x27;,\n",
       "                                                   &#x27;au_...\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=None, n_jobs=None,\n",
       "                               num_parallel_tree=None, random_state=321, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, SimpleImputer(strategy=&#x27;median&#x27;),\n",
       "                                 [&#x27;aa_000&#x27;, &#x27;ac_000&#x27;, &#x27;ae_000&#x27;, &#x27;af_000&#x27;,\n",
       "                                  &#x27;ag_000&#x27;, &#x27;ag_001&#x27;, &#x27;ag_002&#x27;, &#x27;ag_003&#x27;,\n",
       "                                  &#x27;ag_004&#x27;, &#x27;ag_005&#x27;, &#x27;ag_006&#x27;, &#x27;ag_007&#x27;,\n",
       "                                  &#x27;ag_008&#x27;, &#x27;ag_009&#x27;, &#x27;ah_000&#x27;, &#x27;ai_000&#x27;,\n",
       "                                  &#x27;aj_000&#x27;, &#x27;ak_000&#x27;, &#x27;al_000&#x27;, &#x27;am_0&#x27;,\n",
       "                                  &#x27;an_000&#x27;, &#x27;ao_000&#x27;, &#x27;ap_000&#x27;, &#x27;aq_000&#x27;,\n",
       "                                  &#x27;ar_000&#x27;, &#x27;as_000&#x27;, &#x27;at_000&#x27;, &#x27;au_000&#x27;,\n",
       "                                  &#x27;av_000&#x27;, &#x27;ax_000&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">num</label><div class=\"sk-toggleable__content \"><pre>[&#x27;aa_000&#x27;, &#x27;ac_000&#x27;, &#x27;ae_000&#x27;, &#x27;af_000&#x27;, &#x27;ag_000&#x27;, &#x27;ag_001&#x27;, &#x27;ag_002&#x27;, &#x27;ag_003&#x27;, &#x27;ag_004&#x27;, &#x27;ag_005&#x27;, &#x27;ag_006&#x27;, &#x27;ag_007&#x27;, &#x27;ag_008&#x27;, &#x27;ag_009&#x27;, &#x27;ah_000&#x27;, &#x27;ai_000&#x27;, &#x27;aj_000&#x27;, &#x27;ak_000&#x27;, &#x27;al_000&#x27;, &#x27;am_0&#x27;, &#x27;an_000&#x27;, &#x27;ao_000&#x27;, &#x27;ap_000&#x27;, &#x27;aq_000&#x27;, &#x27;ar_000&#x27;, &#x27;as_000&#x27;, &#x27;at_000&#x27;, &#x27;au_000&#x27;, &#x27;av_000&#x27;, &#x27;ax_000&#x27;, &#x27;ay_000&#x27;, &#x27;ay_001&#x27;, &#x27;ay_002&#x27;, &#x27;ay_003&#x27;, &#x27;ay_004&#x27;, &#x27;ay_005&#x27;, &#x27;ay_006&#x27;, &#x27;ay_007&#x27;, &#x27;ay_008&#x27;, &#x27;ay_009&#x27;, &#x27;az_000&#x27;, &#x27;az_001&#x27;, &#x27;az_002&#x27;, &#x27;az_003&#x27;, &#x27;az_004&#x27;, &#x27;az_005&#x27;, &#x27;az_006&#x27;, &#x27;az_007&#x27;, &#x27;az_008&#x27;, &#x27;az_009&#x27;, &#x27;ba_000&#x27;, &#x27;ba_001&#x27;, &#x27;ba_002&#x27;, &#x27;ba_003&#x27;, &#x27;ba_004&#x27;, &#x27;ba_005&#x27;, &#x27;ba_006&#x27;, &#x27;ba_007&#x27;, &#x27;ba_008&#x27;, &#x27;ba_009&#x27;, &#x27;bb_000&#x27;, &#x27;bc_000&#x27;, &#x27;bd_000&#x27;, &#x27;be_000&#x27;, &#x27;bf_000&#x27;, &#x27;bg_000&#x27;, &#x27;bh_000&#x27;, &#x27;bi_000&#x27;, &#x27;bj_000&#x27;, &#x27;bs_000&#x27;, &#x27;bt_000&#x27;, &#x27;bu_000&#x27;, &#x27;bv_000&#x27;, &#x27;bx_000&#x27;, &#x27;by_000&#x27;, &#x27;bz_000&#x27;, &#x27;ca_000&#x27;, &#x27;cb_000&#x27;, &#x27;cc_000&#x27;, &#x27;ce_000&#x27;, &#x27;ci_000&#x27;, &#x27;cj_000&#x27;, &#x27;ck_000&#x27;, &#x27;cn_000&#x27;, &#x27;cn_001&#x27;, &#x27;cn_002&#x27;, &#x27;cn_003&#x27;, &#x27;cn_004&#x27;, &#x27;cn_005&#x27;, &#x27;cn_006&#x27;, &#x27;cn_007&#x27;, &#x27;cn_008&#x27;, &#x27;cn_009&#x27;, &#x27;cp_000&#x27;, &#x27;cq_000&#x27;, &#x27;cs_000&#x27;, &#x27;cs_001&#x27;, &#x27;cs_002&#x27;, &#x27;cs_003&#x27;, &#x27;cs_004&#x27;, &#x27;cs_005&#x27;, &#x27;cs_006&#x27;, &#x27;cs_007&#x27;, &#x27;cs_008&#x27;, &#x27;cs_009&#x27;, &#x27;dd_000&#x27;, &#x27;de_000&#x27;, &#x27;df_000&#x27;, &#x27;dg_000&#x27;, &#x27;dh_000&#x27;, &#x27;di_000&#x27;, &#x27;dj_000&#x27;, &#x27;dk_000&#x27;, &#x27;dl_000&#x27;, &#x27;dm_000&#x27;, &#x27;dn_000&#x27;, &#x27;do_000&#x27;, &#x27;dp_000&#x27;, &#x27;dq_000&#x27;, &#x27;dr_000&#x27;, &#x27;ds_000&#x27;, &#x27;dt_000&#x27;, &#x27;du_000&#x27;, &#x27;dv_000&#x27;, &#x27;dx_000&#x27;, &#x27;dy_000&#x27;, &#x27;dz_000&#x27;, &#x27;ea_000&#x27;, &#x27;eb_000&#x27;, &#x27;ee_000&#x27;, &#x27;ee_001&#x27;, &#x27;ee_002&#x27;, &#x27;ee_003&#x27;, &#x27;ee_004&#x27;, &#x27;ee_005&#x27;, &#x27;ee_006&#x27;, &#x27;ee_007&#x27;, &#x27;ee_008&#x27;, &#x27;ee_009&#x27;, &#x27;ef_000&#x27;, &#x27;eg_000&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">SMOTE</label><div class=\"sk-toggleable__content \"><pre>SMOTE(random_state=42)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler(with_mean=False)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;VarianceThreshold<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_selection.VarianceThreshold.html\">?<span>Documentation for VarianceThreshold</span></a></label><div class=\"sk-toggleable__content \"><pre>VarianceThreshold(threshold=0.01)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">SmartCorrelatedSelection</label><div class=\"sk-toggleable__content \"><pre>SmartCorrelatedSelection(selection_method=&#x27;variance&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;PCA<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a></label><div class=\"sk-toggleable__content \"><pre>PCA(n_components=38)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">XGBClassifier</label><div class=\"sk-toggleable__content \"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=321, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  SimpleImputer(strategy='median'),\n",
       "                                                  ['aa_000', 'ac_000', 'ae_000',\n",
       "                                                   'af_000', 'ag_000', 'ag_001',\n",
       "                                                   'ag_002', 'ag_003', 'ag_004',\n",
       "                                                   'ag_005', 'ag_006', 'ag_007',\n",
       "                                                   'ag_008', 'ag_009', 'ah_000',\n",
       "                                                   'ai_000', 'aj_000', 'ak_000',\n",
       "                                                   'al_000', 'am_0', 'an_000',\n",
       "                                                   'ao_000', 'ap_000', 'aq_000',\n",
       "                                                   'ar_000', 'as_000', 'at_000',\n",
       "                                                   'au_...\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=None, n_jobs=None,\n",
       "                               num_parallel_tree=None, random_state=321, ...))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = best_score[1][1]\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae93f6-531d-484e-87ca-6f08592206b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BayesSearchCV AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "be78d486-0313-47c9-a1e8-1a25e421799a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T21:01:11.943123Z",
     "iopub.status.busy": "2024-07-17T21:01:11.942493Z",
     "iopub.status.idle": "2024-07-17T21:09:54.091023Z",
     "shell.execute_reply": "2024-07-17T21:09:54.090637Z",
     "shell.execute_reply.started": "2024-07-17T21:01:11.943095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/diogo/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparmetros: OrderedDict([('classifier__learning_rate', 0.005761264639821054), ('classifier__n_estimators', 198)])\n",
      "Melhor score: 0.9637065059454304\n"
     ]
    }
   ],
   "source": [
    "## BayesSearchCV para o modelo que obteve o melhor recall\n",
    "\n",
    "param_space = {\n",
    "    'classifier__n_estimators': Integer(10, 200),\n",
    "    'classifier__learning_rate': Real(0.005, 1.0, prior='log-uniform')\n",
    "}\n",
    "\n",
    "cross_validation = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=pipe,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,\n",
    "    cv=cross_validation,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train_trans)\n",
    "\n",
    "print(f\"Melhores hiperparmetros: {bayes_search.best_params_}\")\n",
    "print(\"Melhor score:\", bayes_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67edb9-26ff-4a7f-ac22-9f3d1dfecce3",
   "metadata": {},
   "source": [
    "### BayesSearchCV GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f58f205e-9ac8-4792-a659-e6e9f492fa2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:44:52.094966Z",
     "iopub.status.busy": "2024-07-18T13:44:52.094580Z",
     "iopub.status.idle": "2024-07-18T14:34:11.671275Z",
     "shell.execute_reply": "2024-07-18T14:34:11.670723Z",
     "shell.execute_reply.started": "2024-07-18T13:44:52.094947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparmetros: OrderedDict([('classifier__max_depth', 3), ('classifier__max_features', 2), ('classifier__min_samples_leaf', 5), ('classifier__min_samples_split', 2), ('classifier__n_estimators', 50), ('classifier__subsample', 1.0), ('dim_reduction_algorithm__n_components', 39), ('feature_selection__threshold', 0.8011859032983073), ('threshold__threshold', 0.0)])\n",
      "Melhor score: 0.9530595804263227\n"
     ]
    }
   ],
   "source": [
    "param_space = {\n",
    "    'classifier__n_estimators': (50, 300),\n",
    "    'classifier__max_depth': (3, 10),\n",
    "    'classifier__min_samples_split': (2, 10),\n",
    "    'classifier__min_samples_leaf': (1, 5),\n",
    "    'classifier__subsample': (0.5, 1.0),\n",
    "    'classifier__max_features': (1, 10),\n",
    "    'threshold__threshold': (0.0, 0.3),\n",
    "    'feature_selection__threshold': (0.6, 0.9),\n",
    "    'dim_reduction_algorithm__n_components': (7, 46)\n",
    "}\n",
    "\n",
    "cross_validation = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=pipe,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=96,\n",
    "    cv=cross_validation,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "bayes_search.fit(X, y_trans)\n",
    "\n",
    "print(f\"Melhores hiperparmetros: {bayes_search.best_params_}\")\n",
    "print(\"Melhor score:\", bayes_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a6ee098-9b05-4ac7-bf20-b00262c245a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:26:40.203777Z",
     "iopub.status.busy": "2024-07-18T15:26:40.203509Z",
     "iopub.status.idle": "2024-07-18T15:26:40.208206Z",
     "shell.execute_reply": "2024-07-18T15:26:40.207874Z",
     "shell.execute_reply.started": "2024-07-18T15:26:40.203764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.636939956807921e-05,\n",
       " 3.932226531178352e-05,\n",
       " 0.00021995653635713359,\n",
       " 0.0009159653591828926,\n",
       " 0.0009414841114395624,\n",
       " 0.0009733995717052631,\n",
       " 0.0010533393499105453,\n",
       " 0.0012956018330217204,\n",
       " 0.0013206861927186214,\n",
       " 0.001623445432835893,\n",
       " 0.0017610053334570952,\n",
       " 0.002297970283762487,\n",
       " 0.002939531231877121,\n",
       " 0.0035259907970646943,\n",
       " 0.004194281558337868,\n",
       " 0.004226974069250084,\n",
       " 0.004401509332170982,\n",
       " 0.011163954281730825,\n",
       " 0.01118423685599239,\n",
       " 0.011357674299116138,\n",
       " 0.01993853912506562,\n",
       " 0.020141737091203062,\n",
       " 0.021232735181512356,\n",
       " 0.02224085292270237,\n",
       " 0.023888255021643257,\n",
       " 0.02867857234526542,\n",
       " 0.0314993895747587,\n",
       " 0.032107774264925594,\n",
       " 0.03363702284255324,\n",
       " 0.0359118293775486,\n",
       " 0.03824426324646107,\n",
       " 0.03997823470278959,\n",
       " 0.04557500952796276,\n",
       " 0.046285220192907275,\n",
       " 0.05219038443590691,\n",
       " 0.05278834461079996,\n",
       " 0.05778485416735611,\n",
       " 0.07168481154406722,\n",
       " 0.26071947172975946]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Feature Importance do melhor modelo (39 features no total por causa da dimenso reduzida):\n",
    "\n",
    "best_pipeline = bayes_search.best_estimator_\n",
    "best_classifier = best_pipeline.named_steps['classifier']\n",
    "feature_importances = best_classifier.feature_importances_\n",
    "sorted(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0027d-b099-4cbd-bece-3e78f5f4bd27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T23:03:45.342212Z",
     "iopub.status.busy": "2024-07-17T23:03:45.341856Z",
     "iopub.status.idle": "2024-07-17T23:03:45.344642Z",
     "shell.execute_reply": "2024-07-17T23:03:45.344239Z",
     "shell.execute_reply.started": "2024-07-17T23:03:45.342198Z"
    }
   },
   "source": [
    "10 Falsos Negativos, 187 Verdadeiros Positivos, 11044 Verdadeiros Negativos e 760 Falsos Positivos\n",
    "\n",
    "- Observao: Um Falso Negativo equivale a 50 Falsos Positivos de prejuzo para a empresa\n",
    "- O modelo tentou diminuir ao mximo o nmero de Falsos Positivos\n",
    "- Se aumentasse em 10 o nmero de Falsos Negativos, seria preciso ter no mnimo 260 Falsos Positivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdfdb1-f82f-467b-b8eb-c48065ec997d",
   "metadata": {},
   "source": [
    "### Testando o melhor modelo nos dados de teste "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6f4b9-06a3-4a63-a421-5813b187e741",
   "metadata": {},
   "source": [
    "As seguintes clulas precisam ser automatizadas em funes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4111addb-6a3f-46c8-9df6-ec6e482dbcf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T23:25:18.095600Z",
     "iopub.status.busy": "2024-07-18T23:25:18.095414Z",
     "iopub.status.idle": "2024-07-18T23:25:18.406001Z",
     "shell.execute_reply": "2024-07-18T23:25:18.405636Z",
     "shell.execute_reply.started": "2024-07-18T23:25:18.095590Z"
    }
   },
   "outputs": [],
   "source": [
    "df_present = pd.read_csv(\"air_system_present_year.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cea3841c-abf7-4451-94c5-2df1ed8f2f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T17:36:34.392679Z",
     "iopub.status.busy": "2024-07-18T17:36:34.392230Z",
     "iopub.status.idle": "2024-07-18T17:36:34.395188Z",
     "shell.execute_reply": "2024-07-18T17:36:34.394782Z",
     "shell.execute_reply.started": "2024-07-18T17:36:34.392665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 171)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_present.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46f55661-8e23-415c-9b4d-55d586130446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T17:37:20.759167Z",
     "iopub.status.busy": "2024-07-18T17:37:20.758848Z",
     "iopub.status.idle": "2024-07-18T17:37:20.761789Z",
     "shell.execute_reply": "2024-07-18T17:37:20.761432Z",
     "shell.execute_reply.started": "2024-07-18T17:37:20.759151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ed_000', 'cl_000', 'cm_000', 'ec_00', 'dc_000', 'db_000', 'da_000',\n",
       "       'cz_000', 'cy_000', 'cu_000', 'cv_000', 'ct_000', 'cx_000', 'ad_000',\n",
       "       'ch_000', 'cg_000', 'co_000', 'cf_000', 'bk_000', 'bl_000', 'bm_000',\n",
       "       'bn_000', 'ab_000', 'cr_000', 'bo_000', 'bp_000', 'bq_000', 'br_000'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea42d903-79e0-45df-be8e-d9f40938d562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:27:47.659577Z",
     "iopub.status.busy": "2024-07-18T15:27:47.659157Z",
     "iopub.status.idle": "2024-07-18T15:27:47.799047Z",
     "shell.execute_reply": "2024-07-18T15:27:47.798701Z",
     "shell.execute_reply.started": "2024-07-18T15:27:47.659563Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in df_present.columns:\n",
    "    df_present[i] = df_present[i].replace('na', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "167397ca-ce3a-4bcd-8174-47783904e27c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:27:49.100019Z",
     "iopub.status.busy": "2024-07-18T15:27:49.099806Z",
     "iopub.status.idle": "2024-07-18T15:27:49.125302Z",
     "shell.execute_reply": "2024-07-18T15:27:49.125021Z",
     "shell.execute_reply.started": "2024-07-18T15:27:49.100006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 143)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_present = df_present.drop(columns=cols_to_drop)\n",
    "df_present.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75587a38-5854-44c8-b17f-4c0375325f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:27:59.171916Z",
     "iopub.status.busy": "2024-07-18T15:27:59.171640Z",
     "iopub.status.idle": "2024-07-18T15:27:59.313142Z",
     "shell.execute_reply": "2024-07-18T15:27:59.312824Z",
     "shell.execute_reply.started": "2024-07-18T15:27:59.171902Z"
    }
   },
   "outputs": [],
   "source": [
    "for var in numerical_cols:\n",
    "    df_present[var] = df_present[var].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cde265cd-0974-48c2-85a3-59d575f44555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T23:25:20.727628Z",
     "iopub.status.busy": "2024-07-18T23:25:20.727278Z",
     "iopub.status.idle": "2024-07-18T23:25:20.744101Z",
     "shell.execute_reply": "2024-07-18T23:25:20.743780Z",
     "shell.execute_reply.started": "2024-07-18T23:25:20.727611Z"
    }
   },
   "outputs": [],
   "source": [
    "X_present = df_present.drop(columns=\"class\", axis=1)\n",
    "y_present = df_present[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73c51cce-ae9d-4482-ba45-d2178b9a8d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:28:10.882912Z",
     "iopub.status.busy": "2024-07-18T15:28:10.882725Z",
     "iopub.status.idle": "2024-07-18T15:28:10.889099Z",
     "shell.execute_reply": "2024-07-18T15:28:10.888829Z",
     "shell.execute_reply.started": "2024-07-18T15:28:10.882899Z"
    }
   },
   "outputs": [],
   "source": [
    "y_present_trans = label_transform(y_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "56af53a7-c10e-481b-bf31-df97fb9eda17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T01:37:36.609174Z",
     "iopub.status.busy": "2024-07-19T01:37:36.609008Z",
     "iopub.status.idle": "2024-07-19T01:37:36.612082Z",
     "shell.execute_reply": "2024-07-19T01:37:36.611765Z",
     "shell.execute_reply.started": "2024-07-19T01:37:36.609162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"aa_000\":60,\"ab_000\":\"0\",\"ac_000\":\"20\",\"ad_000\":\"12\",\"ae_000\":\"0\",\"af_000\":\"0\",\"ag_000\":\"0\",\"ag_001\":\"0\",\"ag_002\":\"0\",\"ag_003\":\"2682\",\"ag_004\":\"4736\",\"ag_005\":\"3862\",\"ag_006\":\"1846\",\"ag_007\":\"0\",\"ag_008\":\"0\",\"ag_009\":\"0\",\"ah_000\":\"3976\",\"ai_000\":\"0\",\"aj_000\":\"0\",\"ak_000\":\"0\",\"al_000\":\"1520\",\"am_0\":\"2374\",\"an_000\":\"11516\",\"ao_000\":\"9480\",\"ap_000\":\"111258\",\"aq_000\":\"470\",\"ar_000\":\"0\",\"as_000\":\"0\",\"at_000\":\"0\",\"au_000\":\"0\",\"av_000\":\"58\",\"ax_000\":\"26\",\"ay_000\":\"0\",\"ay_001\":\"0\",\"ay_002\":\"0\",\"ay_003\":\"0\",\"ay_004\":\"0\",\"ay_005\":\"0\",\"ay_006\":\"0\",\"ay_007\":\"0\",\"ay_008\":\"13124\",\"ay_009\":\"2\",\"az_000\":\"1956\",\"az_001\":\"434\",\"az_002\":\"76\",\"az_003\":\"174\",\"az_004\":\"190\",\"az_005\":\"6198\",\"az_006\":\"1148\",\"az_007\":\"2948\",\"az_008\":\"2\",\"az_009\":\"0\",\"ba_000\":\"8762\",\"ba_001\":\"2566\",\"ba_002\":\"480\",\"ba_003\":\"380\",\"ba_004\":\"196\",\"ba_005\":\"516\",\"ba_006\":\"86\",\"ba_007\":\"66\",\"ba_008\":\"74\",\"ba_009\":\"0\",\"bb_000\":\"124340\",\"bc_000\":\"4\",\"bd_000\":\"6\",\"be_000\":\"4\",\"bf_000\":\"0\",\"bg_000\":\"3976\",\"bh_000\":\"318\",\"bi_000\":\"107662\",\"bj_000\":\"3458\",\"bk_000\":\"na\",\"bl_000\":\"na\",\"bm_000\":\"na\",\"bn_000\":\"na\",\"bo_000\":\"na\",\"bp_000\":\"na\",\"bq_000\":\"na\",\"br_000\":\"na\",\"bs_000\":\"110980\",\"bt_000\":\"59.53\",\"bu_000\":\"124340\",\"bv_000\":\"124340\",\"bx_000\":\"184970\",\"by_000\":\"54\",\"bz_000\":\"686\",\"ca_000\":\"12246\",\"cb_000\":\"123880\",\"cc_000\":\"13196\",\"cd_000\":\"1209600\",\"ce_000\":\"598\",\"cf_000\":\"0\",\"cg_000\":\"6\",\"ch_000\":\"0\",\"ci_000\":\"5913.6\",\"cj_000\":\"0\",\"ck_000\":\"5851.2\",\"cl_000\":\"6\",\"cm_000\":\"30\",\"cn_000\":\"0\",\"cn_001\":\"2\",\"cn_002\":\"5512\",\"cn_003\":\"4674\",\"cn_004\":\"1388\",\"cn_005\":\"1508\",\"cn_006\":\"38\",\"cn_007\":\"4\",\"cn_008\":\"0\",\"cn_009\":\"0\",\"co_000\":\"8\",\"cp_000\":\"12\",\"cq_000\":\"124340\",\"cr_000\":\"0\",\"cs_000\":\"1550\",\"cs_001\":\"14\",\"cs_002\":\"36\",\"cs_003\":\"26\",\"cs_004\":\"920\",\"cs_005\":\"430\",\"cs_006\":\"7650\",\"cs_007\":\"2294\",\"cs_008\":\"206\",\"cs_009\":\"0\",\"ct_000\":\"22\",\"cu_000\":\"42\",\"cv_000\":\"5336\",\"cx_000\":\"1276\",\"cy_000\":\"0\",\"cz_000\":\"0\",\"da_000\":\"0\",\"db_000\":\"0\",\"dc_000\":\"6598\",\"dd_000\":\"70\",\"de_000\":\"112\",\"df_000\":\"0\",\"dg_000\":\"0\",\"dh_000\":\"0\",\"di_000\":\"0\",\"dj_000\":\"0\",\"dk_000\":\"0\",\"dl_000\":\"0\",\"dm_000\":\"0\",\"dn_000\":\"340\",\"do_000\":\"0\",\"dp_000\":\"0\",\"dq_000\":\"1100\",\"dr_000\":\"574\",\"ds_000\":\"232\",\"dt_000\":\"66\",\"du_000\":\"780\",\"dv_000\":\"882\",\"dx_000\":\"0\",\"dy_000\":\"4\",\"dz_000\":\"0\",\"ea_000\":\"0\",\"eb_000\":\"0\",\"ec_00\":\"465.5\",\"ed_000\":\"90\",\"ee_000\":\"7502\",\"ee_001\":\"3156\",\"ee_002\":\"1098\",\"ee_003\":\"138\",\"ee_004\":\"412\",\"ee_005\":\"654\",\"ee_006\":\"78\",\"ee_007\":\"88\",\"ee_008\":\"0\",\"ee_009\":\"0\",\"ef_000\":\"0\",\"eg_000\":\"0\"},{\"aa_000\":82,\"ab_000\":\"0\",\"ac_000\":\"68\",\"ad_000\":\"40\",\"ae_000\":\"0\",\"af_000\":\"0\",\"ag_000\":\"0\",\"ag_001\":\"0\",\"ag_002\":\"0\",\"ag_003\":\"0\",\"ag_004\":\"748\",\"ag_005\":\"12594\",\"ag_006\":\"3636\",\"ag_007\":\"0\",\"ag_008\":\"0\",\"ag_009\":\"0\",\"ah_000\":\"5244\",\"ai_000\":\"0\",\"aj_000\":\"60\",\"ak_000\":\"0\",\"al_000\":\"0\",\"am_0\":\"0\",\"an_000\":\"23174\",\"ao_000\":\"18166\",\"ap_000\":\"23686\",\"aq_000\":\"1270\",\"ar_000\":\"0\",\"as_000\":\"0\",\"at_000\":\"0\",\"au_000\":\"0\",\"av_000\":\"12\",\"ax_000\":\"82\",\"ay_000\":\"0\",\"ay_001\":\"0\",\"ay_002\":\"0\",\"ay_003\":\"0\",\"ay_004\":\"0\",\"ay_005\":\"0\",\"ay_006\":\"0\",\"ay_007\":\"692\",\"ay_008\":\"16286\",\"ay_009\":\"0\",\"az_000\":\"280\",\"az_001\":\"44\",\"az_002\":\"50\",\"az_003\":\"1274\",\"az_004\":\"866\",\"az_005\":\"3362\",\"az_006\":\"11102\",\"az_007\":\"0\",\"az_008\":\"0\",\"az_009\":\"0\",\"ba_000\":\"12564\",\"ba_001\":\"1756\",\"ba_002\":\"638\",\"ba_003\":\"276\",\"ba_004\":\"172\",\"ba_005\":\"132\",\"ba_006\":\"812\",\"ba_007\":\"308\",\"ba_008\":\"192\",\"ba_009\":\"128\",\"bb_000\":\"46894\",\"bc_000\":\"4\",\"bd_000\":\"2\",\"be_000\":\"38\",\"bf_000\":\"0\",\"bg_000\":\"5244\",\"bh_000\":\"360\",\"bi_000\":\"20520\",\"bj_000\":\"3134\",\"bk_000\":\"na\",\"bl_000\":\"na\",\"bm_000\":\"na\",\"bn_000\":\"na\",\"bo_000\":\"na\",\"bp_000\":\"na\",\"bq_000\":\"na\",\"br_000\":\"na\",\"bs_000\":\"23320\",\"bt_000\":\"81.89\",\"bu_000\":\"46894\",\"bv_000\":\"46894\",\"bx_000\":\"48324\",\"by_000\":\"68\",\"bz_000\":\"0\",\"ca_000\":\"4486\",\"cb_000\":\"46480\",\"cc_000\":\"17050\",\"cd_000\":\"1209600\",\"ce_000\":\"726\",\"cf_000\":\"2\",\"cg_000\":\"4\",\"ch_000\":\"0\",\"ci_000\":\"7224.96\",\"cj_000\":\"0\",\"ck_000\":\"7768.32\",\"cl_000\":\"0\",\"cm_000\":\"42\",\"cn_000\":\"0\",\"cn_001\":\"0\",\"cn_002\":\"4\",\"cn_003\":\"7064\",\"cn_004\":\"6200\",\"cn_005\":\"2452\",\"cn_006\":\"1246\",\"cn_007\":\"12\",\"cn_008\":\"0\",\"cn_009\":\"0\",\"co_000\":\"14\",\"cp_000\":\"54\",\"cq_000\":\"46894\",\"cr_000\":\"0\",\"cs_000\":\"2202\",\"cs_001\":\"28\",\"cs_002\":\"114\",\"cs_003\":\"350\",\"cs_004\":\"700\",\"cs_005\":\"1708\",\"cs_006\":\"9622\",\"cs_007\":\"2174\",\"cs_008\":\"80\",\"cs_009\":\"0\",\"ct_000\":\"80\",\"cu_000\":\"206\",\"cv_000\":\"7802\",\"cx_000\":\"1466\",\"cy_000\":\"0\",\"cz_000\":\"0\",\"da_000\":\"0\",\"db_000\":\"0\",\"dc_000\":\"7918\",\"dd_000\":\"78\",\"de_000\":\"40\",\"df_000\":\"0\",\"dg_000\":\"0\",\"dh_000\":\"0\",\"di_000\":\"0\",\"dj_000\":\"0\",\"dk_000\":\"0\",\"dl_000\":\"0\",\"dm_000\":\"0\",\"dn_000\":\"352\",\"do_000\":\"0\",\"dp_000\":\"0\",\"dq_000\":\"3996\",\"dr_000\":\"584\",\"ds_000\":\"200\",\"dt_000\":\"62\",\"du_000\":\"37580\",\"dv_000\":\"3756\",\"dx_000\":\"6368\",\"dy_000\":\"36\",\"dz_000\":\"0\",\"ea_000\":\"0\",\"eb_000\":\"0\",\"ec_00\":\"2.86\",\"ed_000\":\"102\",\"ee_000\":\"10040\",\"ee_001\":\"3310\",\"ee_002\":\"1068\",\"ee_003\":\"276\",\"ee_004\":\"1620\",\"ee_005\":\"116\",\"ee_006\":\"86\",\"ee_007\":\"462\",\"ee_008\":\"0\",\"ee_009\":\"0\",\"ef_000\":\"0\",\"eg_000\":\"0\"}]'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_present.iloc[0:2,:].to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d3e1059-c107-454a-9e43-cee447c0f08d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:28:12.075948Z",
     "iopub.status.busy": "2024-07-18T15:28:12.075609Z",
     "iopub.status.idle": "2024-07-18T15:28:12.121469Z",
     "shell.execute_reply": "2024-07-18T15:28:12.121060Z",
     "shell.execute_reply.started": "2024-07-18T15:28:12.075936Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pres_pred = bayes_search.predict(X_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df0a5016-662a-496a-b970-da354ad82b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:44:24.448999Z",
     "iopub.status.busy": "2024-07-19T00:44:24.448734Z",
     "iopub.status.idle": "2024-07-19T00:44:24.451438Z",
     "shell.execute_reply": "2024-07-19T00:44:24.451145Z",
     "shell.execute_reply.started": "2024-07-19T00:44:24.448986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pres_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53019a0a-ce3e-4a94-bf97-f02f6ede0f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:28:13.075664Z",
     "iopub.status.busy": "2024-07-18T15:28:13.075466Z",
     "iopub.status.idle": "2024-07-18T15:28:13.082215Z",
     "shell.execute_reply": "2024-07-18T15:28:13.081874Z",
     "shell.execute_reply.started": "2024-07-18T15:28:13.075649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.99911438 0.27403482]\n",
      "Recall: [0.938624   0.96533333]\n",
      "F1 Score: [0.96792503 0.42688679]\n",
      "Support: [15625   375]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score, support = precision_recall_fscore_support(y_present_trans, y_pres_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Support:\", support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4836024e-1a68-4e99-975a-7358484cdc4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:28:25.499929Z",
     "iopub.status.busy": "2024-07-18T15:28:25.499733Z",
     "iopub.status.idle": "2024-07-18T15:28:25.502118Z",
     "shell.execute_reply": "2024-07-18T15:28:25.501819Z",
     "shell.execute_reply.started": "2024-07-18T15:28:25.499915Z"
    }
   },
   "outputs": [],
   "source": [
    "best_pipeline = bayes_search.best_estimator_\n",
    "best_classifier = best_pipeline.named_steps['classifier'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df0e4161-ba49-4d97-9e1c-4ea2682e3c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:28:32.001636Z",
     "iopub.status.busy": "2024-07-18T15:28:32.001470Z",
     "iopub.status.idle": "2024-07-18T15:28:32.082010Z",
     "shell.execute_reply": "2024-07-18T15:28:32.081705Z",
     "shell.execute_reply.started": "2024-07-18T15:28:32.001624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_present = confusion_matrix(y_present_trans, y_pres_pred, labels=best_classifier)\n",
    "disp_present = ConfusionMatrixDisplay(confusion_matrix=cm_present, display_labels=best_classifier)\n",
    "_ = disp_present.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86f491-5339-4cea-b2f5-dcd7cb9475e2",
   "metadata": {},
   "source": [
    "1466 Verdadeiros Negativos, 13 Falsos Negativos, 959 Falsos Positivos e 362 Verdadeiros Positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4dde2631-f8dc-4aca-af6c-49d8cdc5b5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:46:28.304641Z",
     "iopub.status.busy": "2024-07-18T15:46:28.304297Z",
     "iopub.status.idle": "2024-07-18T15:46:28.306530Z",
     "shell.execute_reply": "2024-07-18T15:46:28.306261Z",
     "shell.execute_reply.started": "2024-07-18T15:46:28.304631Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_total_cost(confusion_matrix):\n",
    "    return (confusion_matrix[1][0] * 500) + (confusion_matrix[1][1] * 10) + (confusion_matrix[1][1] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d24baaea-2018-435c-8ae2-67c8df4ed2ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:46:29.889839Z",
     "iopub.status.busy": "2024-07-18T15:46:29.889628Z",
     "iopub.status.idle": "2024-07-18T15:46:29.892220Z",
     "shell.execute_reply": "2024-07-18T15:46:29.891953Z",
     "shell.execute_reply.started": "2024-07-18T15:46:29.889826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13740"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_total_cost(cm_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b6cbd-8b4f-4e98-8271-dbb9df68e04b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:59:06.492466Z",
     "iopub.status.busy": "2024-07-18T15:59:06.492141Z",
     "iopub.status.idle": "2024-07-18T15:59:06.497794Z",
     "shell.execute_reply": "2024-07-18T15:59:06.497286Z",
     "shell.execute_reply.started": "2024-07-18T15:59:06.492454Z"
    }
   },
   "source": [
    "Supondo que o gasto total da empresa com manuteno de sistemas de ar de caminhes do presente ano fosse parecido com o de 2020 (sem um modelo de Machine Learning), ento a empresa economizaria aproximadamente um total de 23260 dlares em um ano!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4dbd0c-cf24-4716-a0c8-585fd6ec67d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "19f0c677-8165-4ca2-8860-4953d303b602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T23:55:20.992734Z",
     "iopub.status.busy": "2024-07-17T23:55:20.992461Z",
     "iopub.status.idle": "2024-07-17T23:55:21.226164Z",
     "shell.execute_reply": "2024-07-17T23:55:21.225719Z",
     "shell.execute_reply.started": "2024-07-17T23:55:20.992723Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 94394) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[232], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m X_preprocessed \u001b[38;5;241m=\u001b[39m pipeline_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[1;32m      4\u001b[0m X_over_sampled \u001b[38;5;241m=\u001b[39m pipeline_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalancer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfit_resample(X_preprocessed, y_train_trans)\n\u001b[0;32m----> 5\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscaler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_over_sampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m X_var_thresholded \u001b[38;5;241m=\u001b[39m pipeline_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(X_scaled)\n\u001b[1;32m      7\u001b[0m X_selected \u001b[38;5;241m=\u001b[39m pipeline_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_selection\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(X_var_thresholded)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 94394) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "## Tentando obter as variveis em que o modelo julgou serem as mais importantes depois do PCA reduzir a dimensionalidade\n",
    "\n",
    "pipeline_ = bayes_search.best_estimator_\n",
    "\n",
    "X_preprocessed = pipeline_.named_steps['preprocessor'].transform(X_train)\n",
    "X_over_sampled = pipeline_.named_steps['balancer'].fit_resample(X_preprocessed, y_train_trans)\n",
    "X_scaled = pipeline_.named_steps['scaler'].transform(X_over_sampled)\n",
    "X_var_thresholded = pipeline_.named_steps['var_threshold'].transform(X_scaled)\n",
    "X_selected = pipeline_.named_steps['feature_selection'].transform(X_var_thresholded)\n",
    "X_reducted = pipeline_.named_steps['dim_reduction_algorithm'].transform(X_selected)\n",
    "\n",
    "X_inverted = pipeline_.named_steps['dim_reduction_algorithm'].inverse_transform(X_reduced)\n",
    "X_inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70707878-0713-4aa3-8347-411c5e6e1d6f",
   "metadata": {},
   "source": [
    "### Exportando melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb8d3b00-caeb-456d-a41c-a586ec28f535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:35:13.955197Z",
     "iopub.status.busy": "2024-07-18T15:35:13.954842Z",
     "iopub.status.idle": "2024-07-18T15:35:14.024080Z",
     "shell.execute_reply": "2024-07-18T15:35:14.023784Z",
     "shell.execute_reply.started": "2024-07-18T15:35:13.955183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pk']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bayes_search, 'model.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5227c9cf-726e-4441-aa98-4c3f09cf2feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T15:35:52.391057Z",
     "iopub.status.busy": "2024-07-18T15:35:52.390607Z",
     "iopub.status.idle": "2024-07-18T15:35:52.460094Z",
     "shell.execute_reply": "2024-07-18T15:35:52.459796Z",
     "shell.execute_reply.started": "2024-07-18T15:35:52.391039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bayes_search, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c6f3c-6043-43bc-ab33-2c70f51cea8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
